import gc
import json
import logging
import multiprocessing
import os
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
from jesse.helpers import date_to_timestamp

from research.model_pick.candle_fetch import FusionCandles, bar_container
from research.model_pick.feature_select import FeatureSelector
from research.model_pick.features import FeatureLoader
from research.model_pick.labeler import PipelineLabeler
from research.model_pick.model_tuning import ModelTuning

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# æŠ‘åˆ¶Optunaçš„è¯¦ç»†æ—¥å¿—
import optuna

optuna.logging.set_verbosity(optuna.logging.WARNING)

# æŠ‘åˆ¶LightGBMçš„æ—¥å¿—
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# ç”¨äºä¿å­˜deep ssmä¸lg ssm
# ä½¿ç”¨ path = MODEL_SAVE_DIR / "deep_ssm"
# path.resolve().as_posix()çš„æ–¹å¼ç”Ÿæˆè·¯å¾„
MODEL_SAVE_DIR = Path("strategies/BinanceBtcDemoBarV2/models")

# å›ºå®šè®­ç»ƒé›†åˆ‡åˆ†ç‚¹ï¼Œä»è€Œå›ºå®šè®­ç»ƒé›†ï¼ŒèŠ‚çº¦ç‰¹å¾ç”Ÿæˆå’Œç­›é€‰çš„æ—¶é—´ã€‚æµ‹è¯•é›†ä¸»è¦ç”¨äºå›æµ‹
TRAIN_TEST_SPLIT_DATE = "2025-05-31"
CANDLE_START = "2022-08-01"
CANDLE_END = "2025-11-15"
RESULTS_FILE = "model_search_results.csv"


class ModelSearchTracker:
    """ç®¡ç†æ¨¡å‹æœç´¢ç»“æœçš„ä¿å­˜å’Œè¿›åº¦è¿½è¸ª"""

    def __init__(self, results_file: str = RESULTS_FILE):
        self.results_file = results_file
        self.results_df = self._load_results()

    def _load_results(self) -> pd.DataFrame:
        """åŠ è½½å·²æœ‰çš„ç»“æœæ–‡ä»¶"""
        if os.path.exists(self.results_file):
            try:
                df = pd.read_csv(self.results_file)
                logger.info(
                    f"åŠ è½½å·²æœ‰ç»“æœæ–‡ä»¶: {self.results_file}, åŒ…å« {len(df)} æ¡è®°å½•"
                )
                return df
            except Exception as e:
                logger.warning(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}, åˆ›å»ºæ–°æ–‡ä»¶")
                return pd.DataFrame()
        else:
            logger.info(f"åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶: {self.results_file}")
            return pd.DataFrame()

    def is_completed(
        self, log_return_lag: int, pred_next: int, model_type: str
    ) -> bool:
        """æ£€æŸ¥æŸä¸ªå‚æ•°ç»„åˆæ˜¯å¦å·²å®Œæˆ"""
        if self.results_df.empty:
            return False

        mask = (
            (self.results_df["log_return_lag"] == log_return_lag)
            & (self.results_df["pred_next"] == pred_next)
            & (self.results_df["model_type"] == model_type)
            & (self.results_df["status"] == "completed")
        )
        return mask.any()

    def save_result(
        self,
        log_return_lag: int,
        pred_next: int,
        model_type: str,
        best_score: float,
        best_params: dict,
        feature_count: int,
        feature_names: list[str],
        duration: float,
        status: str = "completed",
    ):
        """ä¿å­˜å•ä¸ªå®éªŒç»“æœ"""
        result = {
            "log_return_lag": log_return_lag,
            "pred_next": pred_next,
            "model_type": model_type,
            "best_score": best_score,
            "feature_count": feature_count,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "status": status,
            "duration_seconds": duration,
            "best_params": json.dumps(best_params),
            "selected_features": json.dumps(
                feature_names
            ),  # å°†ç‰¹å¾åˆ—è¡¨ä¿å­˜ä¸ºJSONå­—ç¬¦ä¸²
        }

        # æ·»åŠ åˆ°DataFrame
        new_df = pd.DataFrame([result])
        self.results_df = pd.concat([self.results_df, new_df], ignore_index=True)

        # ä¿å­˜åˆ°æ–‡ä»¶
        self.results_df.to_csv(self.results_file, index=False)
        logger.info(
            f"ä¿å­˜ç»“æœ: {model_type} (lag={log_return_lag}, pred={pred_next}) -> score={best_score:.4f}"
        )

    def get_pending_tasks(self, all_lags: list, all_preds: list) -> list:
        """è·å–æœªå®Œæˆçš„ä»»åŠ¡åˆ—è¡¨"""
        pending = []
        for lag in all_lags:
            for pred in all_preds:
                for model_type in ["regressor", "classifier"]:
                    if not self.is_completed(lag, pred, model_type):
                        pending.append((lag, pred, model_type))
        return pending

    def print_summary(self):
        """æ‰“å°ç»“æœæ±‡æ€»"""
        if self.results_df.empty:
            logger.info("æš‚æ— ç»“æœ")
            return

        print("\n" + "=" * 60)
        print("æ¨¡å‹æœç´¢ç»“æœæ±‡æ€»")
        print("=" * 60)

        # æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„æ˜¾ç¤ºæœ€ä½³ç»“æœ
        for model_type in ["classifier", "regressor"]:
            type_df = self.results_df[self.results_df["model_type"] == model_type]
            if not type_df.empty:
                # åˆ†ç±»å™¨å’Œå›å½’å™¨ï¼ˆRÂ²ï¼‰éƒ½æ˜¯è¶Šå¤§è¶Šå¥½
                best_row = type_df.loc[type_df["best_score"].idxmax()]
                print(f"\n{model_type.upper()} æœ€ä½³æ¨¡å‹:")
                print(f"  - Log Return Lag: {int(best_row['log_return_lag'])}")
                print(f"  - Pred Next: {int(best_row['pred_next'])}")
                print(f"  - Score: {best_row['best_score']:.4f}")
                print(f"  - Features: {int(best_row['feature_count'])}")

        print("\n" + "=" * 60)


logger.info("=" * 60)
logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å’Œç‰¹å¾å¤„ç†æ¨¡å—")
logger.info("=" * 60)

logger.info("åŠ è½½Kçº¿æ•°æ®: Binance Perpetual Futures BTC-USDT 1m")
candle_container = FusionCandles(
    exchange="Binance Perpetual Futures", symbol="BTC-USDT", timeframe="1m"
)
logger.info(f"{bar_container.THRESHOLD = }")
candles = candle_container.get_candles(CANDLE_START, CANDLE_END)
logger.info(f"Kçº¿æ•°æ®åŠ è½½å®Œæˆ: {len(candles)} æ¡è®°å½•")
logger.info(
    f"æ—¶é—´èŒƒå›´: {pd.to_datetime(candles[0][0], unit='ms')} - {pd.to_datetime(candles[-1][0], unit='ms')}"
)

# ç‰¹å¾ç”Ÿæˆåªå…³å¿ƒç‰¹å¾åç§°å’ŒåŸå§‹æ•°æ®
logger.info("åˆå§‹åŒ–ç‰¹å¾åŠ è½½å™¨...")
feature_loader = FeatureLoader(candles)

# ç”±äºè®­ç»ƒé›†ç›¸åŒï¼Œselectorå†…éƒ¨çš„deep ssmä¸lg ssmåªéœ€è¦è®­ç»ƒä¸€æ¬¡
logger.info("åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨ï¼ˆå°†ç¼“å­˜SSMæ¨¡å‹ï¼‰...")
feature_selector = FeatureSelector(model_save_dir=MODEL_SAVE_DIR)
logger.info("åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–è¿½è¸ªå™¨
tracker = ModelSearchTracker()


def cleanup_multiprocessing_resources():
    """
    å¼ºåˆ¶æ¸…ç† multiprocessing èµ„æºï¼Œé˜²æ­¢ç´¯ç§¯æ³„æ¼

    è¿™ä¸ªå‡½æ•°è§£å†³çš„é—®é¢˜ï¼š
    - LightGBM + GridSearchCV åˆ›å»ºçš„ worker è¿›ç¨‹æ± 
    - è¿›ç¨‹é—´é€šä¿¡çš„ semaphore å’Œ shared memory
    - è¿™äº›èµ„æºåœ¨ä»»åŠ¡ç»“æŸåå¯èƒ½ä¸ä¼šè‡ªåŠ¨é‡Šæ”¾
    """
    # 1. å¼ºåˆ¶ Python åƒåœ¾å›æ”¶
    gc.collect()

    # 2. æ¸…ç† multiprocessing çš„å…¨å±€èµ„æº
    try:
        # è·å–å½“å‰è¿›ç¨‹çš„æ‰€æœ‰å­è¿›ç¨‹
        current_process = multiprocessing.current_process()

        # å¦‚æœå­˜åœ¨æ´»è·ƒçš„å­è¿›ç¨‹ï¼Œç­‰å¾…å®ƒä»¬ç»“æŸ
        for child in multiprocessing.active_children():
            child.join(timeout=0.1)  # çŸ­æš‚ç­‰å¾…
            if child.is_alive():
                child.terminate()  # å¼ºåˆ¶ç»ˆæ­¢åƒµå°¸è¿›ç¨‹

        # 3. å†æ¬¡åƒåœ¾å›æ”¶ï¼Œæ¸…ç†ç»ˆæ­¢è¿›ç¨‹çš„èµ„æº
        gc.collect()

    except Exception as e:
        logger.warning(f"æ¸…ç† multiprocessing èµ„æºæ—¶å‡ºç°è­¦å‘Šï¼ˆå¯å¿½ç•¥ï¼‰: {e}")

    logger.debug("âœ“ Multiprocessing èµ„æºæ¸…ç†å®Œæˆ")


def evaluate_classifier(
    candles: np.ndarray,
    log_return_lag: int,
    pred_next: int,
):
    logger.info(
        f"[åˆ†ç±»å™¨] å¼€å§‹è¯„ä¼° - log_return_lag={log_return_lag}, pred_next={pred_next}"
    )

    # åˆ›å»ºæ ‡ç­¾
    logger.info(f"[åˆ†ç±»å™¨] åˆ›å»ºæ ‡ç­¾å™¨ï¼Œlog_return_lag={log_return_lag}")
    labeler = PipelineLabeler(candles, log_return_lag)
    label_for_classifier = labeler.label_hard
    logger.info(
        f"[åˆ†ç±»å™¨] æ ‡ç­¾åˆ†å¸ƒ: {np.unique(label_for_classifier, return_counts=True)}"
    )

    # è·å–ç‰¹å¾å’Œæ ‡ç­¾
    logger.info(f"[åˆ†ç±»å™¨] åŠ è½½ç‰¹å¾æ•°æ®ï¼Œpred_next={pred_next}")
    df_feat, label_c = feature_loader.get_feature_label_bundle(
        label_for_classifier, pred_next
    )
    logger.info(f"[åˆ†ç±»å™¨] ç‰¹å¾ç»´åº¦: {df_feat.shape}")

    # åˆ’åˆ†è®­ç»ƒé›†
    train_mask = df_feat.index.to_numpy() < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    train_x_all_feat = df_feat[train_mask]
    train_y = label_c[train_mask]
    logger.info(
        f"[åˆ†ç±»å™¨] è®­ç»ƒé›†å¤§å°: {train_x_all_feat.shape[0]} æ ·æœ¬, {train_x_all_feat.shape[1]} ç‰¹å¾"
    )
    logger.info(
        f"[åˆ†ç±»å™¨] è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(*np.unique(train_y, return_counts=True)))}"
    )

    # ç‰¹å¾é€‰æ‹©
    logger.info(f"[åˆ†ç±»å™¨] å¼€å§‹ç‰¹å¾é€‰æ‹©...")
    feature_names = feature_selector.select_features(train_x_all_feat, train_y)
    logger.info(
        f"[åˆ†ç±»å™¨] ç‰¹å¾é€‰æ‹©å®Œæˆ: ä» {train_x_all_feat.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {len(feature_names)} ä¸ª"
    )
    logger.debug(f"[åˆ†ç±»å™¨] é€‰ä¸­çš„ç‰¹å¾: {feature_names[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾

    # æ¨¡å‹è°ƒå‚
    logger.info(f"[åˆ†ç±»å™¨] å¼€å§‹æ¨¡å‹è°ƒå‚...")
    model_tuning = ModelTuning(
        TRAIN_TEST_SPLIT_DATE,
        train_x_all_feat,
        train_y,
    )

    params, best_score = model_tuning.tuning_classifier(feature_selector, feature_names)
    logger.info(f"[åˆ†ç±»å™¨] è°ƒå‚å®Œæˆ - æœ€ä½³å¾—åˆ†: {best_score:.4f}")
    logger.info(f"[åˆ†ç±»å™¨] æœ€ä½³å‚æ•°: {params}")

    return params, best_score, len(feature_names), feature_names


def evaluate_regressor(
    candles: np.ndarray,
    log_return_lag: int,
    pred_next: int,
):
    logger.info(
        f"[å›å½’å™¨] å¼€å§‹è¯„ä¼° - log_return_lag={log_return_lag}, pred_next={pred_next}"
    )

    # åˆ›å»ºæ ‡ç­¾
    logger.info(f"[å›å½’å™¨] åˆ›å»ºæ ‡ç­¾å™¨ï¼Œlog_return_lag={log_return_lag}")
    labeler = PipelineLabeler(candles, log_return_lag)
    label_for_regressor = labeler.label_direction
    logger.info(
        f"[å›å½’å™¨] æ ‡ç­¾ç»Ÿè®¡: å‡å€¼={np.mean(label_for_regressor):.6f}, æ ‡å‡†å·®={np.std(label_for_regressor):.6f}"
    )

    # è·å–ç‰¹å¾å’Œæ ‡ç­¾
    logger.info(f"[å›å½’å™¨] åŠ è½½ç‰¹å¾æ•°æ®ï¼Œpred_next={pred_next}")
    df_feat, label_r = feature_loader.get_feature_label_bundle(
        label_for_regressor, pred_next
    )
    logger.info(f"[å›å½’å™¨] ç‰¹å¾ç»´åº¦: {df_feat.shape}")

    # åˆ’åˆ†è®­ç»ƒé›†
    train_mask = df_feat.index.to_numpy() < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    train_x_all_feat = df_feat[train_mask]
    train_y = label_r[train_mask]
    logger.info(
        f"[å›å½’å™¨] è®­ç»ƒé›†å¤§å°: {train_x_all_feat.shape[0]} æ ·æœ¬, {train_x_all_feat.shape[1]} ç‰¹å¾"
    )
    logger.info(
        f"[å›å½’å™¨] è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: [{np.min(train_y):.6f}, {np.max(train_y):.6f}]"
    )

    # ç‰¹å¾é€‰æ‹©
    logger.info(f"[å›å½’å™¨] å¼€å§‹ç‰¹å¾é€‰æ‹©...")
    feature_names = feature_selector.select_features(train_x_all_feat, train_y)
    logger.info(
        f"[å›å½’å™¨] ç‰¹å¾é€‰æ‹©å®Œæˆ: ä» {train_x_all_feat.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {len(feature_names)} ä¸ª"
    )
    logger.debug(f"[å›å½’å™¨] é€‰ä¸­çš„ç‰¹å¾: {feature_names[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾

    # æ¨¡å‹è°ƒå‚
    logger.info(f"[å›å½’å™¨] å¼€å§‹æ¨¡å‹è°ƒå‚...")
    model_tuning = ModelTuning(
        TRAIN_TEST_SPLIT_DATE,
        train_x_all_feat,
        train_y,
    )

    params, best_score = model_tuning.tuning_regressor(feature_selector, feature_names)
    logger.info(f"[å›å½’å™¨] è°ƒå‚å®Œæˆ - æœ€ä½³RÂ²å¾—åˆ†: {best_score:.4f}")
    logger.info(f"[å›å½’å™¨] æœ€ä½³å‚æ•°: {params}")

    return params, best_score, len(feature_names), feature_names


if __name__ == "__main__":
    # å‚æ•°é…ç½®
    log_return_lags = list(range(4, 8))
    pred_next_steps = [1, 2, 3, 4]

    # è·å–å¾…å®Œæˆçš„ä»»åŠ¡
    logger.info("\n" + "=" * 60)
    logger.info("ä»»åŠ¡è§„åˆ’")
    logger.info("=" * 60)
    logger.info(f"å‚æ•°é…ç½®:")
    logger.info(f"  - log_return_lags: {log_return_lags}")
    logger.info(f"  - pred_next_steps: {pred_next_steps}")
    logger.info(f"  - æ¨¡å‹ç±»å‹: ['classifier', 'regressor']")
    logger.info(f"  - è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ—¥æœŸ: {TRAIN_TEST_SPLIT_DATE}")

    pending_tasks = tracker.get_pending_tasks(log_return_lags, pred_next_steps)
    total_tasks = len(log_return_lags) * len(pred_next_steps) * 2  # 2ç§æ¨¡å‹ç±»å‹
    completed_tasks = total_tasks - len(pending_tasks)

    logger.info(f"\nä»»åŠ¡ç»Ÿè®¡:")
    logger.info(f"  - æ€»ä»»åŠ¡æ•°: {total_tasks}")
    logger.info(f"  - å·²å®Œæˆ: {completed_tasks}")
    logger.info(f"  - å¾…å®Œæˆ: {len(pending_tasks)}")

    if pending_tasks:
        logger.info(f"\nå¾…å®Œæˆä»»åŠ¡åˆ—è¡¨:")
        for i, (lag, pred, model_type) in enumerate(
            pending_tasks[:5], 1
        ):  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"  {i}. {model_type}: lag={lag}, pred={pred}")
        if len(pending_tasks) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(pending_tasks) - 5} ä¸ªä»»åŠ¡")

    if len(pending_tasks) == 0:
        logger.info("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ!")
        tracker.print_summary()
        exit(0)

    # ä¸»å¾ªç¯
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹æ¨¡å‹æœç´¢ä¸»å¾ªç¯")
    logger.info("=" * 60)

    for task_idx, (lag, pred, model_type) in enumerate(pending_tasks, 1):
        # æ˜¾ç¤ºè¿›åº¦
        overall_progress = completed_tasks + task_idx
        logger.info("\n" + "-" * 60)
        logger.info(
            f"[è¿›åº¦ {overall_progress}/{total_tasks}] ({(overall_progress - 1) / total_tasks * 100:.1f}%) ä»»åŠ¡ #{task_idx}/{len(pending_tasks)}"
        )
        logger.info(
            f"å¼€å§‹è®­ç»ƒ: {model_type.upper()} | log_return_lag={lag} | pred_next={pred}"
        )
        logger.info("-" * 60)

        try:
            start_time = time.time()

            if model_type == "classifier":
                params, score, feature_count, feature_names = evaluate_classifier(
                    candles.copy(), lag, pred
                )
            else:
                params, score, feature_count, feature_names = evaluate_regressor(
                    candles.copy(), lag, pred
                )

            duration = time.time() - start_time

            # ä¿å­˜ç»“æœ
            tracker.save_result(
                log_return_lag=lag,
                pred_next=pred,
                model_type=model_type,
                best_score=score,
                best_params=params,
                feature_count=feature_count,
                feature_names=feature_names,
                duration=duration,
                status="completed",
            )

            logger.info("\n" + "=" * 40)
            logger.info(f"âœ“ ä»»åŠ¡å®Œæˆï¼")
            logger.info(f"  - æ¨¡å‹ç±»å‹: {model_type}")
            logger.info(f"  - å‚æ•°: lag={lag}, pred={pred}")
            logger.info(f"  - æœ€ä½³å¾—åˆ†: {score:.4f}")
            logger.info(f"  - ç‰¹å¾æ•°é‡: {feature_count}")
            logger.info(f"  - è®­ç»ƒè€—æ—¶: {duration:.1f} ç§’")
            logger.info(
                f"  - é¢„è®¡å‰©ä½™æ—¶é—´: {(len(pending_tasks) - task_idx) * duration / 60:.1f} åˆ†é’Ÿ"
            )
            logger.info("=" * 40)

            # ğŸ”§ å¼ºåˆ¶æ¸…ç†èµ„æºï¼Œé˜²æ­¢å¤šè¿›ç¨‹èµ„æºæ³„æ¼ç´¯ç§¯
            cleanup_multiprocessing_resources()

        except KeyboardInterrupt:
            logger.warning("\n" + "!" * 60)
            logger.warning("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            logger.warning(
                f"å½“å‰è¿›åº¦: {overall_progress}/{total_tasks} ({overall_progress / total_tasks * 100:.1f}%)"
            )
            logger.warning("!" * 60)
            tracker.print_summary()
            exit(0)

        except Exception as e:
            logger.error("\n" + "!" * 60)
            logger.error(f"âœ— è®­ç»ƒå¤±è´¥!")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"  - å¤±è´¥ä»»åŠ¡: {model_type} (lag={lag}, pred={pred})")
            logger.error(f"  - å½“å‰è¿›åº¦: {overall_progress}/{total_tasks}")
            logger.error("!" * 60)
            logger.error("ç¨‹åºç»ˆæ­¢ï¼Œæ˜¾ç¤ºå·²å®Œæˆçš„ç»“æœï¼š")
            # æ˜¾ç¤ºå·²å®Œæˆçš„ç»“æœ
            tracker.print_summary()
            raise

    # å®Œæˆåæ˜¾ç¤ºæ±‡æ€»
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    logger.info("=" * 60)
    tracker.print_summary()
    logger.info("\nç¨‹åºæ‰§è¡Œå®Œæ¯•")
