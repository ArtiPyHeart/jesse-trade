"""
Pipeline Find Model - ä½¿ç”¨ FeaturePipeline çš„æ¨¡å‹æœç´¢æµæ°´çº¿

æ–°æµç¨‹ï¼š
1. è·å– fusion candles
2. å…¨å±€ FeaturePipelineï¼ˆä¸é™ç»´ï¼‰â†’ è®¡ç®—å…¨é‡ç‰¹å¾ï¼ˆå« SSMï¼‰
3. æŒ‰ label è¿›è¡Œç‰¹å¾ç­›é€‰ â†’ è¿”å›ç‰¹å¾åç§°ï¼ˆå« SSM å¦‚ deep_ssm_0ï¼‰
4. æ¨¡å‹ç‰¹å®š FeaturePipelineï¼ˆcopy_ssm_from + é™ç»´ï¼‰â†’ é™ç»´åç‰¹å¾
5. LightGBM è°ƒå‚
6. CSV è®°å½•ï¼ˆå«é™ç»´å™¨é…ç½®ã€é™ç»´å‰ç‰¹å¾æ•°é‡ï¼‰
"""

import gc
import json
import logging
import multiprocessing
import os
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
from jesse.helpers import date_to_timestamp

from research.model_pick.candle_fetch import FusionCandles, bar_container
from research.model_pick.feature_utils import (
    align_features_and_labels,
    build_full_feature_config,
    build_model_config,
    select_features,
)
from research.model_pick.features import ALL_FEATS
from research.model_pick.labeler import PipelineLabeler
from research.model_pick.model_tuning import ModelTuning
from src.features.pipeline import FeaturePipeline

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# æŠ‘åˆ¶Optunaçš„è¯¦ç»†æ—¥å¿—
import optuna

optuna.logging.set_verbosity(optuna.logging.WARNING)

# æŠ‘åˆ¶LightGBMçš„æ—¥å¿—
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# ç”¨äºä¿å­˜deep ssmä¸lg ssm
MODEL_SAVE_DIR = Path("strategies/BinanceBtcDemoBarV2/models")

# å›ºå®šè®­ç»ƒé›†åˆ‡åˆ†ç‚¹
TRAIN_TEST_SPLIT_DATE = "2025-05-31"
CANDLE_START = "2022-08-01"
CANDLE_END = "2025-07-01"
RESULTS_FILE = "model_search_results.csv"

# ARDVAE é™ç»´å™¨é…ç½®ï¼ˆå›ºå®šï¼Œä¸è¿›è¡Œè°ƒå‚ï¼‰
REDUCER_CONFIG = {
    "max_latent_dim": 512,  # over-complete è®¾è®¡ï¼ŒARD prior è‡ªåŠ¨ç¡®å®š active dims
    "kl_threshold": 0.01,  # åˆ¤æ–­ç»´åº¦æ˜¯å¦ active çš„é˜ˆå€¼
    "max_epochs": 200,
    "patience": 15,
    "seed": 42,
}


class ModelSearchTracker:
    """ç®¡ç†æ¨¡å‹æœç´¢ç»“æœçš„ä¿å­˜å’Œè¿›åº¦è¿½è¸ª"""

    def __init__(self, results_file: str = RESULTS_FILE):
        self.results_file = results_file
        self.results_df = self._load_results()

    def _load_results(self) -> pd.DataFrame:
        """åŠ è½½å·²æœ‰çš„ç»“æœæ–‡ä»¶"""
        if os.path.exists(self.results_file):
            try:
                df = pd.read_csv(self.results_file)
                logger.info(
                    f"åŠ è½½å·²æœ‰ç»“æœæ–‡ä»¶: {self.results_file}, åŒ…å« {len(df)} æ¡è®°å½•"
                )
                return df
            except Exception as e:
                logger.warning(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}, åˆ›å»ºæ–°æ–‡ä»¶")
                return pd.DataFrame()
        else:
            logger.info(f"åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶: {self.results_file}")
            return pd.DataFrame()

    def is_completed(
        self, log_return_lag: int, pred_next: int, model_type: str
    ) -> bool:
        """æ£€æŸ¥æŸä¸ªå‚æ•°ç»„åˆæ˜¯å¦å·²å®Œæˆ"""
        if self.results_df.empty:
            return False

        mask = (
            (self.results_df["log_return_lag"] == log_return_lag)
            & (self.results_df["pred_next"] == pred_next)
            & (self.results_df["model_type"] == model_type)
            & (self.results_df["status"] == "completed")
        )
        return mask.any()

    def save_result(
        self,
        log_return_lag: int,
        pred_next: int,
        model_type: str,
        best_score: float,
        best_params: dict,
        feature_count: int,
        feature_names: list[str],
        duration: float,
        reducer_config: dict,
        n_features_before_reduction: int,
        n_features_after_reduction: int,
        status: str = "completed",
    ):
        """ä¿å­˜å•ä¸ªå®éªŒç»“æœï¼ˆæ–°å¢é™ç»´ç›¸å…³å­—æ®µï¼‰"""
        result = {
            "log_return_lag": log_return_lag,
            "pred_next": pred_next,
            "model_type": model_type,
            "best_score": best_score,
            "feature_count": feature_count,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "status": status,
            "duration_seconds": duration,
            "selected_features": json.dumps(feature_names),
            # æ–°å¢å­—æ®µï¼šé™ç»´å™¨ç›¸å…³
            "reducer_config": json.dumps(reducer_config),
            "n_features_before_reduction": n_features_before_reduction,
            "n_features_after_reduction": n_features_after_reduction,
            # æ¨¡å‹æœ€ä½³å‚æ•°
            "best_params": json.dumps(best_params),
        }

        # æ·»åŠ åˆ°DataFrame
        new_df = pd.DataFrame([result])
        self.results_df = pd.concat([self.results_df, new_df], ignore_index=True)

        # ä¿å­˜åˆ°æ–‡ä»¶
        self.results_df.to_csv(self.results_file, index=False)
        logger.info(
            f"ä¿å­˜ç»“æœ: {model_type} (lag={log_return_lag}, pred={pred_next}) -> score={best_score:.4f}"
        )

    def get_pending_tasks(self, all_lags: list, all_preds: list) -> list:
        """è·å–æœªå®Œæˆçš„ä»»åŠ¡åˆ—è¡¨"""
        pending = []
        for lag in all_lags:
            for pred in all_preds:
                for model_type in ["regressor", "classifier"]:
                    if not self.is_completed(lag, pred, model_type):
                        pending.append((lag, pred, model_type))
        return pending

    def print_summary(self):
        """æ‰“å°ç»“æœæ±‡æ€»"""
        if self.results_df.empty:
            logger.info("æš‚æ— ç»“æœ")
            return

        print("\n" + "=" * 60)
        print("æ¨¡å‹æœç´¢ç»“æœæ±‡æ€»")
        print("=" * 60)

        # æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„æ˜¾ç¤ºæœ€ä½³ç»“æœ
        for model_type in ["classifier", "regressor"]:
            type_df = self.results_df[self.results_df["model_type"] == model_type]
            if not type_df.empty:
                best_row = type_df.loc[type_df["best_score"].idxmax()]
                print(f"\n{model_type.upper()} æœ€ä½³æ¨¡å‹:")
                print(f"  - Log Return Lag: {int(best_row['log_return_lag'])}")
                print(f"  - Pred Next: {int(best_row['pred_next'])}")
                print(f"  - Score: {best_row['best_score']:.4f}")
                print(
                    f"  - Features (é™ç»´å‰): {int(best_row.get('n_features_before_reduction', best_row['feature_count']))}"
                )
                print(
                    f"  - Features (é™ç»´å): {int(best_row.get('n_features_after_reduction', best_row['feature_count']))}"
                )

        print("\n" + "=" * 60)


def cleanup_multiprocessing_resources():
    """å¼ºåˆ¶æ¸…ç† multiprocessing èµ„æºï¼Œé˜²æ­¢ç´¯ç§¯æ³„æ¼"""
    import ctypes

    # å¤šè½®å¼ºåˆ¶ Python åƒåœ¾å›æ”¶ï¼ˆå¤„ç†å¾ªç¯å¼•ç”¨ï¼‰
    for _ in range(3):
        gc.collect()

    # æ¸…ç† multiprocessing çš„å…¨å±€èµ„æº
    try:
        for child in multiprocessing.active_children():
            child.join(timeout=1.0)
            if child.is_alive():
                child.terminate()
                child.join(timeout=1.0)
        gc.collect()
    except Exception as e:
        logger.warning(f"æ¸…ç† multiprocessing èµ„æºæ—¶å‡ºç°è­¦å‘Šï¼ˆå¯å¿½ç•¥ï¼‰: {e}")

    # å°è¯•é‡Šæ”¾ C åº“å†…å­˜ï¼ˆmacOS/Linuxï¼‰
    try:
        if hasattr(ctypes, "CDLL"):
            libc = ctypes.CDLL("libc.dylib")
            if hasattr(libc, "malloc_trim"):
                libc.malloc_trim(0)
    except Exception:
        pass

    gc.collect()
    logger.debug("âœ“ Multiprocessing èµ„æºæ¸…ç†å®Œæˆ")


logger.info("=" * 60)
logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å’Œç‰¹å¾å¤„ç†æ¨¡å—")
logger.info("=" * 60)

logger.info("åŠ è½½Kçº¿æ•°æ®: Binance Perpetual Futures BTC-USDT 1m")
candle_container = FusionCandles(
    exchange="Binance Perpetual Futures", symbol="BTC-USDT", timeframe="1m"
)
logger.info(f"{bar_container.THRESHOLD = }")
candles = candle_container.get_candles(CANDLE_START, CANDLE_END)
logger.info(f"Kçº¿æ•°æ®åŠ è½½å®Œæˆ: {len(candles)} æ¡è®°å½•")
logger.info(
    f"æ—¶é—´èŒƒå›´: {pd.to_datetime(candles[0][0], unit='ms')} - {pd.to_datetime(candles[-1][0], unit='ms')}"
)

# æ„å»ºå…¨å±€ FeaturePipelineï¼ˆä¸é™ç»´ï¼‰ï¼Œè®¡ç®—å…¨é‡ç‰¹å¾
logger.info("åˆå§‹åŒ–å…¨å±€ FeaturePipelineï¼ˆä¸é™ç»´ï¼‰...")
global_config = build_full_feature_config(ALL_FEATS, ssm_state_dim=5)
global_pipeline = FeaturePipeline(global_config)
logger.info(f"é…ç½®ç‰¹å¾æ•°: {len(global_config.feature_names)} (å« SSM ç‰¹å¾)")

logger.info("è®¡ç®—å…¨å±€ç‰¹å¾ï¼ˆè®­ç»ƒ SSM æ¨¡å‹ï¼‰...")
global_features = global_pipeline.fit_transform(candles)
logger.info(f"å…¨å±€ç‰¹å¾è®¡ç®—å®Œæˆ: {global_features.shape}")

# åˆå§‹åŒ–è¿½è¸ªå™¨
tracker = ModelSearchTracker()


def evaluate_classifier(
    global_pipeline: FeaturePipeline,
    global_features: pd.DataFrame,
    candles: np.ndarray,
    log_return_lag: int,
    pred_next: int,
):
    """
    è¯„ä¼°åˆ†ç±»å™¨

    æµç¨‹ï¼š
    1. ç”Ÿæˆæ ‡ç­¾
    2. å¯¹é½å…¨å±€ç‰¹å¾ä¸æ ‡ç­¾
    3. åˆ’åˆ†è®­ç»ƒé›†
    4. ç‰¹å¾ç­›é€‰
    5. æ„å»ºæ¨¡å‹ç‰¹å®š Pipelineï¼ˆå¯ç”¨é™ç»´ï¼‰
    6. è®¡ç®—é™ç»´åç‰¹å¾
    7. æ¨¡å‹è°ƒå‚
    """
    logger.info(
        f"[åˆ†ç±»å™¨] å¼€å§‹è¯„ä¼° - log_return_lag={log_return_lag}, pred_next={pred_next}"
    )

    # 1. ç”Ÿæˆæ ‡ç­¾
    logger.info(f"[åˆ†ç±»å™¨] åˆ›å»ºæ ‡ç­¾å™¨ï¼Œlog_return_lag={log_return_lag}")
    labeler = PipelineLabeler(candles, log_return_lag)
    raw_label = labeler.label_hard
    logger.info(
        f"[åˆ†ç±»å™¨] æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(*np.unique(raw_label[~np.isnan(raw_label)].astype(int), return_counts=True)))}"
    )

    # 2. å¯¹é½å…¨å±€ç‰¹å¾ä¸æ ‡ç­¾
    logger.info("[åˆ†ç±»å™¨] å¯¹é½ç‰¹å¾å’Œæ ‡ç­¾...")
    aligned_features, aligned_labels = align_features_and_labels(
        global_features, raw_label, pred_next, candles[:, 0]
    )
    logger.info(f"[åˆ†ç±»å™¨] å¯¹é½åç‰¹å¾ç»´åº¦: {aligned_features.shape}")

    # 3. åˆ’åˆ†è®­ç»ƒé›†
    train_mask = aligned_features.index < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    train_x = aligned_features[train_mask]
    train_y = aligned_labels[: train_mask.sum()]
    logger.info(
        f"[åˆ†ç±»å™¨] è®­ç»ƒé›†å¤§å°: {train_x.shape[0]} æ ·æœ¬, {train_x.shape[1]} ç‰¹å¾"
    )

    # 4. ç‰¹å¾ç­›é€‰
    logger.info("[åˆ†ç±»å™¨] å¼€å§‹ç‰¹å¾ç­›é€‰...")
    selection_result = select_features(train_x, train_y)
    logger.info(
        f"[åˆ†ç±»å™¨] ç‰¹å¾ç­›é€‰å®Œæˆ: ä» {selection_result.n_total} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {selection_result.n_selected} ä¸ª"
    )

    # 5. æ„å»ºæ¨¡å‹ç‰¹å®š Pipelineï¼ˆå¯ç”¨é™ç»´ï¼‰
    logger.info("[åˆ†ç±»å™¨] æ„å»ºæ¨¡å‹ç‰¹å®š Pipelineï¼ˆå¯ç”¨ ARDVAE é™ç»´ï¼‰...")
    model_config = build_model_config(
        selection_result.selected_features,
        ssm_state_dim=5,
        reducer_config=REDUCER_CONFIG,
    )
    model_pipeline = FeaturePipeline(model_config)
    model_pipeline.share_raw_calculator_from(global_pipeline)
    model_pipeline.copy_ssm_from(global_pipeline)

    # 6. è®¡ç®—é™ç»´åç‰¹å¾
    logger.info("[åˆ†ç±»å™¨] è®¡ç®—é™ç»´åç‰¹å¾...")
    model_features = model_pipeline.fit_transform(candles)
    logger.info(
        f"[åˆ†ç±»å™¨] é™ç»´å®Œæˆ: {selection_result.n_selected} -> {model_features.shape[1]} ç»´"
    )

    # 7. é‡æ–°å¯¹é½é™ç»´åç‰¹å¾
    model_aligned, _ = align_features_and_labels(
        model_features, raw_label, pred_next, candles[:, 0]
    )
    train_x_reduced = model_aligned[
        model_aligned.index < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    ]

    # 8. æ¨¡å‹è°ƒå‚
    logger.info("[åˆ†ç±»å™¨] å¼€å§‹æ¨¡å‹è°ƒå‚...")
    model_tuning = ModelTuning(TRAIN_TEST_SPLIT_DATE, train_x_reduced, train_y)
    params, best_score = model_tuning.tuning_classifier_direct(train_x_reduced, train_y)
    logger.info(f"[åˆ†ç±»å™¨] è°ƒå‚å®Œæˆ - æœ€ä½³å¾—åˆ†: {best_score:.4f}")

    # è¿”å›ç»“æœ
    reducer_info = {
        "config": REDUCER_CONFIG,
        "n_before_reduction": selection_result.n_selected,
        "n_after_reduction": model_features.shape[1],
    }

    # æ¸…ç†æ¨¡å‹ç‰¹å®š Pipeline
    del model_pipeline
    gc.collect()

    return (
        params,
        best_score,
        selection_result.n_selected,
        selection_result.selected_features,
        reducer_info,
    )


def evaluate_regressor(
    global_pipeline: FeaturePipeline,
    global_features: pd.DataFrame,
    candles: np.ndarray,
    log_return_lag: int,
    pred_next: int,
):
    """
    è¯„ä¼°å›å½’å™¨

    æµç¨‹ä¸åˆ†ç±»å™¨ç›¸åŒï¼Œä½¿ç”¨è¿ç»­æ ‡ç­¾
    """
    logger.info(
        f"[å›å½’å™¨] å¼€å§‹è¯„ä¼° - log_return_lag={log_return_lag}, pred_next={pred_next}"
    )

    # 1. ç”Ÿæˆæ ‡ç­¾
    logger.info(f"[å›å½’å™¨] åˆ›å»ºæ ‡ç­¾å™¨ï¼Œlog_return_lag={log_return_lag}")
    labeler = PipelineLabeler(candles, log_return_lag)
    raw_label = labeler.label_direction
    valid_labels = raw_label[~np.isnan(raw_label)]
    logger.info(
        f"[å›å½’å™¨] æ ‡ç­¾ç»Ÿè®¡: å‡å€¼={np.mean(valid_labels):.6f}, æ ‡å‡†å·®={np.std(valid_labels):.6f}"
    )

    # 2. å¯¹é½å…¨å±€ç‰¹å¾ä¸æ ‡ç­¾
    logger.info("[å›å½’å™¨] å¯¹é½ç‰¹å¾å’Œæ ‡ç­¾...")
    aligned_features, aligned_labels = align_features_and_labels(
        global_features, raw_label, pred_next, candles[:, 0]
    )
    logger.info(f"[å›å½’å™¨] å¯¹é½åç‰¹å¾ç»´åº¦: {aligned_features.shape}")

    # 3. åˆ’åˆ†è®­ç»ƒé›†
    train_mask = aligned_features.index < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    train_x = aligned_features[train_mask]
    train_y = aligned_labels[: train_mask.sum()]
    logger.info(
        f"[å›å½’å™¨] è®­ç»ƒé›†å¤§å°: {train_x.shape[0]} æ ·æœ¬, {train_x.shape[1]} ç‰¹å¾"
    )

    # 4. ç‰¹å¾ç­›é€‰
    logger.info("[å›å½’å™¨] å¼€å§‹ç‰¹å¾ç­›é€‰...")
    selection_result = select_features(train_x, train_y)
    logger.info(
        f"[å›å½’å™¨] ç‰¹å¾ç­›é€‰å®Œæˆ: ä» {selection_result.n_total} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {selection_result.n_selected} ä¸ª"
    )

    # 5. æ„å»ºæ¨¡å‹ç‰¹å®š Pipelineï¼ˆå¯ç”¨é™ç»´ï¼‰
    logger.info("[å›å½’å™¨] æ„å»ºæ¨¡å‹ç‰¹å®š Pipelineï¼ˆå¯ç”¨ ARDVAE é™ç»´ï¼‰...")
    model_config = build_model_config(
        selection_result.selected_features,
        ssm_state_dim=5,
        reducer_config=REDUCER_CONFIG,
    )
    model_pipeline = FeaturePipeline(model_config)
    model_pipeline.share_raw_calculator_from(global_pipeline)
    model_pipeline.copy_ssm_from(global_pipeline)

    # 6. è®¡ç®—é™ç»´åç‰¹å¾
    logger.info("[å›å½’å™¨] è®¡ç®—é™ç»´åç‰¹å¾...")
    model_features = model_pipeline.fit_transform(candles)
    logger.info(
        f"[å›å½’å™¨] é™ç»´å®Œæˆ: {selection_result.n_selected} -> {model_features.shape[1]} ç»´"
    )

    # 7. é‡æ–°å¯¹é½é™ç»´åç‰¹å¾
    model_aligned, _ = align_features_and_labels(
        model_features, raw_label, pred_next, candles[:, 0]
    )
    train_x_reduced = model_aligned[
        model_aligned.index < date_to_timestamp(TRAIN_TEST_SPLIT_DATE)
    ]

    # 8. æ¨¡å‹è°ƒå‚
    logger.info("[å›å½’å™¨] å¼€å§‹æ¨¡å‹è°ƒå‚...")
    model_tuning = ModelTuning(TRAIN_TEST_SPLIT_DATE, train_x_reduced, train_y)
    params, best_score = model_tuning.tuning_regressor_direct(train_x_reduced, train_y)
    logger.info(f"[å›å½’å™¨] è°ƒå‚å®Œæˆ - æœ€ä½³RÂ²å¾—åˆ†: {best_score:.4f}")

    # è¿”å›ç»“æœ
    reducer_info = {
        "config": REDUCER_CONFIG,
        "n_before_reduction": selection_result.n_selected,
        "n_after_reduction": model_features.shape[1],
    }

    # æ¸…ç†æ¨¡å‹ç‰¹å®š Pipeline
    del model_pipeline
    gc.collect()

    return (
        params,
        best_score,
        selection_result.n_selected,
        selection_result.selected_features,
        reducer_info,
    )


if __name__ == "__main__":
    # å‚æ•°é…ç½®
    log_return_lags = list(range(4, 8))
    pred_next_steps = [1, 2, 3]

    # è·å–å¾…å®Œæˆçš„ä»»åŠ¡
    logger.info("\n" + "=" * 60)
    logger.info("ä»»åŠ¡è§„åˆ’")
    logger.info("=" * 60)
    logger.info("å‚æ•°é…ç½®:")
    logger.info(f"  - log_return_lags: {log_return_lags}")
    logger.info(f"  - pred_next_steps: {pred_next_steps}")
    logger.info("  - æ¨¡å‹ç±»å‹: ['classifier', 'regressor']")
    logger.info(f"  - è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ—¥æœŸ: {TRAIN_TEST_SPLIT_DATE}")
    logger.info(
        f"  - é™ç»´å™¨é…ç½®: max_latent_dim={REDUCER_CONFIG['max_latent_dim']}, kl_threshold={REDUCER_CONFIG['kl_threshold']}"
    )

    pending_tasks = tracker.get_pending_tasks(log_return_lags, pred_next_steps)
    total_tasks = len(log_return_lags) * len(pred_next_steps) * 2
    completed_tasks = total_tasks - len(pending_tasks)

    logger.info("\nä»»åŠ¡ç»Ÿè®¡:")
    logger.info(f"  - æ€»ä»»åŠ¡æ•°: {total_tasks}")
    logger.info(f"  - å·²å®Œæˆ: {completed_tasks}")
    logger.info(f"  - å¾…å®Œæˆ: {len(pending_tasks)}")

    if pending_tasks:
        logger.info("\nå¾…å®Œæˆä»»åŠ¡åˆ—è¡¨:")
        for i, (lag, pred, model_type) in enumerate(pending_tasks[:5], 1):
            logger.info(f"  {i}. {model_type}: lag={lag}, pred={pred}")
        if len(pending_tasks) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(pending_tasks) - 5} ä¸ªä»»åŠ¡")

    if len(pending_tasks) == 0:
        logger.info("æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ!")
        tracker.print_summary()
        exit(0)

    # ä¸»å¾ªç¯
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹æ¨¡å‹æœç´¢ä¸»å¾ªç¯")
    logger.info("=" * 60)

    for task_idx, (lag, pred, model_type) in enumerate(pending_tasks, 1):
        # æ˜¾ç¤ºè¿›åº¦
        overall_progress = completed_tasks + task_idx
        logger.info("\n" + "-" * 60)
        logger.info(
            f"[è¿›åº¦ {overall_progress}/{total_tasks}] ({(overall_progress - 1) / total_tasks * 100:.1f}%) ä»»åŠ¡ #{task_idx}/{len(pending_tasks)}"
        )
        logger.info(
            f"å¼€å§‹è®­ç»ƒ: {model_type.upper()} | log_return_lag={lag} | pred_next={pred}"
        )
        logger.info("-" * 60)

        try:
            start_time = time.time()

            if model_type == "classifier":
                params, score, feature_count, feature_names, reducer_info = (
                    evaluate_classifier(
                        global_pipeline,
                        global_features.copy(),
                        candles.copy(),
                        lag,
                        pred,
                    )
                )
            else:
                params, score, feature_count, feature_names, reducer_info = (
                    evaluate_regressor(
                        global_pipeline,
                        global_features.copy(),
                        candles.copy(),
                        lag,
                        pred,
                    )
                )

            duration = time.time() - start_time

            # ä¿å­˜ç»“æœ
            tracker.save_result(
                log_return_lag=lag,
                pred_next=pred,
                model_type=model_type,
                best_score=score,
                best_params=params,
                feature_count=feature_count,
                feature_names=feature_names,
                duration=duration,
                reducer_config=reducer_info["config"],
                n_features_before_reduction=reducer_info["n_before_reduction"],
                n_features_after_reduction=reducer_info["n_after_reduction"],
                status="completed",
            )

            logger.info("\n" + "=" * 40)
            logger.info("âœ“ ä»»åŠ¡å®Œæˆï¼")
            logger.info(f"  - æ¨¡å‹ç±»å‹: {model_type}")
            logger.info(f"  - å‚æ•°: lag={lag}, pred={pred}")
            logger.info(f"  - æœ€ä½³å¾—åˆ†: {score:.4f}")
            logger.info(
                f"  - ç‰¹å¾æ•°é‡: {reducer_info['n_before_reduction']} -> {reducer_info['n_after_reduction']} (é™ç»´å)"
            )
            logger.info(f"  - è®­ç»ƒè€—æ—¶: {duration:.1f} ç§’")
            logger.info(
                f"  - é¢„è®¡å‰©ä½™æ—¶é—´: {(len(pending_tasks) - task_idx) * duration / 60:.1f} åˆ†é’Ÿ"
            )
            logger.info("=" * 40)

            # å¼ºåˆ¶æ¸…ç†èµ„æº
            cleanup_multiprocessing_resources()

        except KeyboardInterrupt:
            logger.warning("\n" + "!" * 60)
            logger.warning("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            logger.warning(
                f"å½“å‰è¿›åº¦: {overall_progress}/{total_tasks} ({overall_progress / total_tasks * 100:.1f}%)"
            )
            logger.warning("!" * 60)
            tracker.print_summary()
            exit(0)

        except Exception as e:
            logger.error("\n" + "!" * 60)
            logger.error("âœ— è®­ç»ƒå¤±è´¥!")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"  - å¤±è´¥ä»»åŠ¡: {model_type} (lag={lag}, pred={pred})")
            logger.error(f"  - å½“å‰è¿›åº¦: {overall_progress}/{total_tasks}")
            logger.error("!" * 60)
            logger.error("ç¨‹åºç»ˆæ­¢ï¼Œæ˜¾ç¤ºå·²å®Œæˆçš„ç»“æœï¼š")
            tracker.print_summary()
            raise

    # å®Œæˆåæ˜¾ç¤ºæ±‡æ€»
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    logger.info("=" * 60)
    tracker.print_summary()
    logger.info("\nç¨‹åºæ‰§è¡Œå®Œæ¯•")
