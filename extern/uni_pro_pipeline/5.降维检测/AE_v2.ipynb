{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adda3b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c552a046f8474e45a2cc5dadf1d91974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='RB99_1m_Train_10877.csv', description='文件路径:', placeholder='输入CSV文件路径'), HBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5495fc6f2a3473a881d794fc3aa8e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "### 设置随机种子，以确保结果可复现\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 检测可用设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 标准\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "        ### 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "### 变分\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, encoding_dim)\n",
    "        self.fc_logvar = nn.Linear(128, encoding_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()  ### 确保输出在0和1范围内\n",
    "        )\n",
    "        ### 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std, device=device)  \n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "\n",
    "    \n",
    "### 循环\n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim, seq_length=20):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        ### 确保input_dim能被seq_length整除\n",
    "        if input_dim % seq_length != 0:\n",
    "            raise ValueError(f\"input_dim ({input_dim}) 必须能被 seq_length ({seq_length}) 整除\")\n",
    "        \n",
    "        self.input_size = input_dim // seq_length\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=self.input_size, \n",
    "            hidden_size=encoding_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=encoding_dim, \n",
    "            hidden_size=self.input_size, \n",
    "            num_layers=1, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        ### 重塑输入以适应LSTM\n",
    "        x_reshaped = x.view(batch_size, self.seq_length, self.input_size)\n",
    "        \n",
    "        ### 编码\n",
    "        _, (h_n, _) = self.encoder(x_reshaped)\n",
    "        encoded = h_n.squeeze(0)  \n",
    "        \n",
    "        ### 解码\n",
    "        decoded_input = encoded.unsqueeze(1).expand(-1, self.seq_length, -1)\n",
    "        decoded_output, _ = self.decoder(decoded_input)\n",
    "        decoded_output = decoded_output.contiguous().view(batch_size, -1)\n",
    "        \n",
    "        return decoded_output\n",
    "\n",
    "\n",
    "### 对抗\n",
    "\n",
    "class AdversarialAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AdversarialAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "        ### 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### 对抗的判别器\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z):\n",
    "        validity = self.model(z)\n",
    "        return validity\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "### 训练标准\n",
    "\n",
    "def train_standard_autoencoder(X, encoding_dim=50, epochs=50, batch_size=64, learning_rate=0.001):\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = Autoencoder(X.shape[1], encoding_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            data = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Standard AE Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_features = model.encoder(X_tensor).cpu().numpy()\n",
    "    \n",
    "    return encoded_features, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 训练变分\n",
    "\n",
    "def train_variational_autoencoder(X, encoding_dim=50, epochs=50, batch_size=64, learning_rate=0.001):\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = VariationalAutoencoder(X.shape[1], encoding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        \n",
    "        ### 确保输入在0和1范围内，适合BCE损失\n",
    "        x_clamped = torch.clamp(x, min=1e-7, max=1-1e-7)\n",
    "        recon_x_clamped = torch.clamp(recon_x, min=1e-7, max=1-1e-7)\n",
    "        \n",
    "        ### 计算重构损失BCE\n",
    "        BCE = nn.functional.binary_cross_entropy(\n",
    "            recon_x_clamped, x_clamped, reduction='sum'\n",
    "        )\n",
    "        \n",
    "        ### 计算KL散度\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        return BCE + KLD\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            data = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'VAE Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        encoded_features = model.encoder(X_tensor)[0].cpu().numpy()     ### 使用均值作为编码\n",
    "    \n",
    "    return encoded_features, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 训练循环\n",
    "\n",
    "def train_recurrent_autoencoder(X, encoding_dim=50, epochs=50, batch_size=64, learning_rate=0.001, seq_length=20):\n",
    "    ### 确保特征维度能被seq_length整除\n",
    "    input_dim = X.shape[1]\n",
    "    if input_dim % seq_length != 0:\n",
    "        ### 调整特征维度以适应seq_length\n",
    "        new_dim = (input_dim // seq_length) * seq_length\n",
    "        print(f\"警告: 特征维度 ({input_dim}) 不能被seq_length ({seq_length}) 整除，将截断至 {new_dim}\")\n",
    "        X = X[:, :new_dim]\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = RecurrentAutoencoder(X.shape[1], encoding_dim, seq_length).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            data = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Recurrent AE Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ### 提取编码特征 (取LSTM的最后一个隐藏状态)\n",
    "        encoded_features = []\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size]\n",
    "            batch_encoded = model.encoder(batch.view(batch.size(0), model.seq_length, -1))[1][0].squeeze(0).cpu().numpy()\n",
    "            encoded_features.append(batch_encoded)\n",
    "        encoded_features = np.vstack(encoded_features)\n",
    "    \n",
    "    return encoded_features, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 训练对抗\n",
    "\n",
    "def train_adversarial_autoencoder(X, encoding_dim=50, epochs=50, batch_size=64, learning_rate=0.001):\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    ae = AdversarialAutoencoder(X.shape[1], encoding_dim).to(device)\n",
    "    discriminator = Discriminator(encoding_dim).to(device)\n",
    "    \n",
    "    ### 优化器\n",
    "    optimizer_ae = optim.Adam(ae.parameters(), lr=learning_rate)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    \n",
    "    ### 损失函数\n",
    "    criterion_ae = nn.MSELoss()\n",
    "    criterion_d = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ae.train()\n",
    "        discriminator.train()\n",
    "        running_ae_loss = 0.0\n",
    "        running_d_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            data = batch[0]\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            ### 训练判别器\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            ### 真实样本 (从标准正态分布采样)\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            z_real = torch.randn(batch_size, encoding_dim, device=device)\n",
    "            d_real = discriminator(z_real)\n",
    "            d_real_loss = criterion_d(d_real, real_labels)\n",
    "            \n",
    "            ### 假样本 (编码器生成)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "            z_fake = ae.encoder(data).detach()  ### 切断梯度\n",
    "            d_fake = discriminator(z_fake)\n",
    "            d_fake_loss = criterion_d(d_fake, fake_labels)\n",
    "            \n",
    "            ### 判别器总损失\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "            running_d_loss += d_loss.item()\n",
    "            \n",
    "            ### 训练自编码器\n",
    "            optimizer_ae.zero_grad()\n",
    "            \n",
    "            ### 重构损失\n",
    "            reconstructed = ae(data)\n",
    "            recon_loss = criterion_ae(reconstructed, data)\n",
    "            \n",
    "            ### 对抗损失\n",
    "            z_fake = ae.encoder(data)\n",
    "            d_fake = discriminator(z_fake)\n",
    "            g_loss = criterion_d(d_fake, real_labels)\n",
    "            \n",
    "            ### 总损失 \n",
    "            ae_loss = recon_loss + 0.01 * g_loss     ### 降低对抗损失权重，防止模式崩溃\n",
    "            ae_loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_ae_loss += ae_loss.item()\n",
    "        \n",
    "        epoch_ae_loss = running_ae_loss / len(dataloader)\n",
    "        epoch_d_loss = running_d_loss / len(dataloader)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Adversarial AE Epoch [{epoch+1}/{epochs}], AE Loss: {epoch_ae_loss:.4f}, D Loss: {epoch_d_loss:.4f}')\n",
    "    \n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_features = ae.encoder(X_tensor).cpu().numpy()\n",
    "    \n",
    "    return encoded_features, ae\n",
    "\n",
    "\n",
    "def evaluate_reconstruction(X, reconstructed):\n",
    "    return mean_squared_error(X, reconstructed)\n",
    "\n",
    "\n",
    "def run_autoencoder_comparison(X, y, output_dir='autoencoder_results', \n",
    "                              encoding_dim=50, epochs=50, batch_size=64,\n",
    "                              use_standard_ae=True, use_vae=True,\n",
    "                              use_recurrent_ae=True, use_adversarial_ae=True,\n",
    "                              use_kernel_pca=True, evaluate=True):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    ### 数据标准化\n",
    "    print(\"标准化数据...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    ###  保存原始数据和标签\n",
    "    np.save(os.path.join(output_dir, 'original_features.npy'), X_scaled)\n",
    "    np.save(os.path.join(output_dir, 'labels.npy'), y)\n",
    "    \n",
    "    results = {}\n",
    "    models = {}\n",
    "\n",
    "    if use_standard_ae:\n",
    "        print(\"\\n训练标准自编码器...\")\n",
    "        encoded_std_ae, model_std_ae = train_standard_autoencoder(\n",
    "            X_scaled, \n",
    "            encoding_dim=encoding_dim,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        results['standard_ae'] = encoded_std_ae\n",
    "        models['standard_ae'] = model_std_ae\n",
    "        np.save(os.path.join(output_dir, 'standard_ae_features.npy'), encoded_std_ae)\n",
    "        \n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                model_std_ae.eval()\n",
    "                reconstructed = model_std_ae(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "            mse = evaluate_reconstruction(X_scaled, reconstructed)\n",
    "            print(f\"标准自编码器重构MSE: {mse:.6f}\")\n",
    "    \n",
    "    if use_vae:\n",
    "        print(\"\\n训练变分自编码器...\")\n",
    "        encoded_vae, model_vae = train_variational_autoencoder(\n",
    "            X_scaled, \n",
    "            encoding_dim=encoding_dim,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        results['vae'] = encoded_vae\n",
    "        models['vae'] = model_vae\n",
    "        np.save(os.path.join(output_dir, 'vae_features.npy'), encoded_vae)\n",
    "        \n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                model_vae.eval()\n",
    "                reconstructed, _, _ = model_vae(torch.FloatTensor(X_scaled).to(device))\n",
    "                reconstructed = reconstructed.cpu().numpy()\n",
    "            mse = evaluate_reconstruction(X_scaled, reconstructed)\n",
    "            print(f\"变分自编码器重构MSE: {mse:.6f}\")\n",
    "    \n",
    "    if use_recurrent_ae:\n",
    "        print(\"\\n训练循环自编码器...\")\n",
    "        seq_length = 20    ### 序列长度\n",
    "        encoded_recurrent_ae, model_recurrent_ae = train_recurrent_autoencoder(\n",
    "            X_scaled, \n",
    "            encoding_dim=encoding_dim,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "        results['recurrent_ae'] = encoded_recurrent_ae\n",
    "        models['recurrent_ae'] = model_recurrent_ae\n",
    "        np.save(os.path.join(output_dir, 'recurrent_ae_features.npy'), encoded_recurrent_ae)\n",
    "        \n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                model_recurrent_ae.eval()\n",
    "                reconstructed = model_recurrent_ae(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "            mse = evaluate_reconstruction(X_scaled, reconstructed)\n",
    "            print(f\"循环自编码器重构MSE: {mse:.6f}\")\n",
    "    \n",
    "    if use_adversarial_ae:\n",
    "        print(\"\\n训练对抗自编码器...\")\n",
    "        encoded_adv_ae, model_adv_ae = train_adversarial_autoencoder(\n",
    "            X_scaled, \n",
    "            encoding_dim=encoding_dim,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        results['adversarial_ae'] = encoded_adv_ae\n",
    "        models['adversarial_ae'] = model_adv_ae\n",
    "        np.save(os.path.join(output_dir, 'adversarial_ae_features.npy'), encoded_adv_ae)\n",
    "        \n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                model_adv_ae.eval()\n",
    "                reconstructed = model_adv_ae(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "            mse = evaluate_reconstruction(X_scaled, reconstructed)\n",
    "            print(f\"对抗自编码器重构MSE: {mse:.6f}\")\n",
    "\n",
    "    if evaluate:\n",
    "        print(\"\\n生成评估报告...\")\n",
    "        report = {}\n",
    "        for method, features in results.items():\n",
    "            \n",
    "            ### 自编码器重构\n",
    "            model = models[method]\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                if method == 'vae':\n",
    "                    reconstructed, _, _ = model(torch.FloatTensor(X_scaled).to(device))\n",
    "                    reconstructed = reconstructed.cpu().numpy()\n",
    "                else:\n",
    "                    reconstructed = model(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "                \n",
    "            mse = evaluate_reconstruction(X_scaled, reconstructed)\n",
    "            report[method] = mse\n",
    "        \n",
    "        report_df = pd.DataFrame(list(report.items()), columns=['方法', '重构MSE'])\n",
    "        report_df.to_csv(os.path.join(output_dir, 'reconstruction_report.csv'), index=False)\n",
    "        print(\"重构评估报告已保存。\")\n",
    "    \n",
    "    print(\"\\n所有降维方法已完成！\")\n",
    "    print(f\"结果已保存至: {output_dir}\")\n",
    "    return results, models, report_df if evaluate else None\n",
    "\n",
    "\n",
    "def create_interactive_ui(file_path='RB99_1m_Train_10877.csv'):  ##### 训练集数据\n",
    "    \n",
    "    ### 基础参数\n",
    "    encoding_dim = widgets.IntText(value=50, description='降维维度:', min=10, max=200)\n",
    "    epochs = widgets.IntText(value=50, description='训练轮数:', min=10, max=200)\n",
    "    batch_size = widgets.IntText(value=64, description='批次大小:', min=32, max=1024)\n",
    "    \n",
    "    file_path_input = widgets.Text(\n",
    "        value=file_path,\n",
    "        description='文件路径:',\n",
    "        placeholder='输入CSV文件路径'\n",
    "    )\n",
    "    \n",
    "    method_options = {\n",
    "        '标准自编码器': widgets.Checkbox(value=True, description='标准自编码器'),\n",
    "        '变分自编码器': widgets.Checkbox(value=True, description='变分自编码器'),\n",
    "        '循环自编码器': widgets.Checkbox(value=True, description='循环自编码器'),\n",
    "        '对抗自编码器': widgets.Checkbox(value=True, description='对抗自编码器')\n",
    "    }\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='运行降维',\n",
    "        button_style='success',\n",
    "        tooltip='点击开始运行自编码器对比分析',\n",
    "        icon='play'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    params_box = widgets.VBox([\n",
    "        file_path_input,\n",
    "        widgets.HBox([encoding_dim, epochs, batch_size]),\n",
    "        widgets.VBox(list(method_options.values())),\n",
    "        run_button\n",
    "    ])\n",
    "    \n",
    "    def on_run_clicked(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            file_path = file_path_input.value\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"错误: 文件 '{file_path}' 不存在！\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(f\"读取数据: {file_path}\")\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"数据形状: {df.shape}\")\n",
    "                \n",
    "                # 提取特征和标签\n",
    "                X = df.iloc[:, 1:].values\n",
    "                y = df.iloc[:, 0].values\n",
    "                \n",
    "                methods = {\n",
    "                    'use_standard_ae': method_options['标准自编码器'].value,\n",
    "                    'use_vae': method_options['变分自编码器'].value,\n",
    "                    'use_recurrent_ae': method_options['循环自编码器'].value,\n",
    "                    'use_adversarial_ae': method_options['对抗自编码器'].value\n",
    "                }\n",
    "                \n",
    "                start_time = time.time()\n",
    "                results, models, report = run_autoencoder_comparison(\n",
    "                    X, y,\n",
    "                    output_dir='autoencoder_results',\n",
    "                    encoding_dim=encoding_dim.value,\n",
    "                    epochs=epochs.value,\n",
    "                    batch_size=batch_size.value,\n",
    "                    **methods\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                print(f\"\\n全部完成！耗时: {end_time - start_time:.2f}秒\")\n",
    "                \n",
    "                if report is not None:\n",
    "                    print(\"\\n重构误差对比:\")\n",
    "                    display(report.sort_values('重构MSE'))\n",
    "            \n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(f\"运行过程中发生错误: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    run_button.on_click(on_run_clicked)\n",
    "        \n",
    "    display(params_box, output)\n",
    "\n",
    "\n",
    "create_interactive_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aaa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8dc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0810e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23962ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89613a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0368b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
