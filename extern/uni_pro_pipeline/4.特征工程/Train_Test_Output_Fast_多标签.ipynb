{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c86221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:44:24.187221Z",
     "start_time": "2025-04-05T04:44:24.160818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12120\n",
      "12115\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_name_1 = 'RB99_1m_Turnover_31000_12120_1213.csv'  ###########  轴文件  ##############\n",
    "df = pd.read_csv(file_name_1)\n",
    "print(len(df))\n",
    "\n",
    "file_name_2 = 'RB99_1m_Turnover_31000_12120_1213_Label_513.csv'  ############# Label 文件 ###########\n",
    "df_label = pd.read_csv(file_name_2)\n",
    "print(len(df_label))\n",
    "\n",
    "date = df['eob'] > '2023-01-01'  ##########  测试集开始日期，格式：xxxx-xx-xx     #############\n",
    "\n",
    "roll = 5  ######## 窗口期\n",
    "\n",
    "label = 30  ######## 如果标签9是26  8是27，7是28，6是29，5是30,15是20\n",
    "\n",
    "move = 1   ###### 默认为 1（学习未来下一个标签） \n",
    "\n",
    "test_df = df[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c5c22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:44:26.505064Z",
     "start_time": "2025-04-05T04:44:26.503163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10877\n",
      "Test: 1213\n"
     ]
    }
   ],
   "source": [
    "Train = len(df) - len(test_df) - 30  #######  减少30个，是为了给特征留出计算的空间\n",
    "print('Train: ' + str(Train))\n",
    "\n",
    "Test = len(test_df)\n",
    "print('Test: ' + str(Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3ccc57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:44:29.558078Z",
     "start_time": "2025-04-05T04:44:29.555396Z"
    }
   },
   "outputs": [],
   "source": [
    "file_80 = file_name_1 + '_tz80.csv'  ############ 80个特征过度文件 ############\n",
    "\n",
    "file_400_Train = file_name_1 + \"_tz80_Train_\" + str(Train) + \".csv\"  ############  训练集文件  #############\n",
    "\n",
    "file_400_Test = file_name_1 + \"_tz80_Test_\" + str(Test) + \".csv\"  #############  测试集文件 ############\n",
    "\n",
    "file_400_Train_3 = file_name_1 + \"_tz80_Test_\" + str(Test) + \"_PCA\" + \".csv\"  #############  测试集针对PCA降维文件 ############\n",
    "\n",
    "file_Train = str(Train) + \".csv\"\n",
    "file_Test = str(Test) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff39b15c29b2f793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:44:31.850426Z",
     "start_time": "2025-04-05T04:44:31.846260Z"
    }
   },
   "outputs": [],
   "source": [
    "df['open'] = df['open'].astype('float64')\n",
    "df['close'] = df['close'].astype('float64')\n",
    "df['high'] = df['high'].astype('float64')\n",
    "df['low'] = df['low'].astype('float64')\n",
    "\n",
    "open1 = df['open']\n",
    "close = df['close']\n",
    "high = df['high']\n",
    "low = df['low']\n",
    "\n",
    "eob = df['eob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754a64cf362dc156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:44:35.729987Z",
     "start_time": "2025-04-05T04:44:35.140700Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['close',\n",
    "                             'log_open1_1', 'log_open1_2', 'log_open1_3', 'log_open1_4',\n",
    "                             'log_open2_1', 'log_open2_2', 'log_open2_3', 'log_open2_4',\n",
    "                             'log_open3_1', 'log_open3_2', 'log_open3_3', 'log_open3_4',\n",
    "                             'log_open4_1', 'log_open4_2', 'log_open4_3', 'log_open4_4',\n",
    "                             'log_open5_1', 'log_open5_2', 'log_open5_3', 'log_open5_4',\n",
    "                             'log_close1_1', 'log_close1_2', 'log_close1_3', 'log_close1_4',\n",
    "                             'log_close2_1', 'log_close2_2', 'log_close2_3', 'log_close2_4',\n",
    "                             'log_close3_1', 'log_close3_2', 'log_close3_3', 'log_close3_4',\n",
    "                             'log_close4_1', 'log_close4_2', 'log_close4_3', 'log_close4_4',\n",
    "                             'log_close5_1', 'log_close5_2', 'log_close5_3', 'log_close5_4',\n",
    "                             'log_high1_1', 'log_high1_2', 'log_high1_3', 'log_high1_4',\n",
    "                             'log_high2_1', 'log_high2_2', 'log_high2_3', 'log_high2_4',\n",
    "                             'log_high3_1', 'log_high3_2', 'log_high3_3', 'log_high3_4',\n",
    "                             'log_high4_1', 'log_high4_2', 'log_high4_3', 'log_high4_4',\n",
    "                             'log_high5_1', 'log_high5_2', 'log_high5_3', 'log_high5_4',\n",
    "                             'log_low1_1', 'log_low1_2', 'log_low1_3', 'log_low1_4',\n",
    "                             'log_low2_1', 'log_low2_2', 'log_low2_3', 'log_low2_4',\n",
    "                             'log_low3_1', 'log_low3_2', 'log_low3_3', 'log_low3_4',\n",
    "                             'log_low4_1', 'log_low4_2', 'log_low4_3', 'log_low4_4',\n",
    "                             'log_low5_1', 'log_low5_2', 'log_low5_3', 'log_low5_4'])\n",
    "\n",
    "log_open = np.log(open1)\n",
    "log_close = np.log(close)\n",
    "log_high = np.log(high)\n",
    "log_low = np.log(low)\n",
    "\n",
    "data['close'] = df['close']\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "data['log_open1_1'] = log_open - log_open.shift(1)\n",
    "data['log_open1_2'] = log_open - log_close.shift(1)\n",
    "data['log_open1_3'] = log_open - log_high.shift(1)\n",
    "data['log_open1_4'] = log_open - log_low.shift(1)\n",
    "\n",
    "data['log_open2_1'] = log_open - log_open.shift(2)\n",
    "data['log_open2_2'] = log_open - log_close.shift(2)\n",
    "data['log_open2_3'] = log_open - log_high.shift(2)\n",
    "data['log_open2_4'] = log_open - log_low.shift(2)\n",
    "\n",
    "data['log_open3_1'] = log_open - log_open.shift(3)\n",
    "data['log_open3_2'] = log_open - log_close.shift(3)\n",
    "data['log_open3_3'] = log_open - log_high.shift(3)\n",
    "data['log_open3_4'] = log_open - log_low.shift(3)\n",
    "\n",
    "data['log_open4_1'] = log_open - log_open.shift(4)\n",
    "data['log_open4_2'] = log_open - log_close.shift(4)\n",
    "data['log_open4_3'] = log_open - log_high.shift(4)\n",
    "data['log_open4_4'] = log_open - log_low.shift(4)\n",
    "\n",
    "data['log_open5_1'] = log_open - log_open.shift(5)\n",
    "data['log_open5_2'] = log_open - log_close.shift(5)\n",
    "data['log_open5_3'] = log_open - log_high.shift(5)\n",
    "data['log_open5_4'] = log_open - log_low.shift(5)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "data['log_close1_1'] = log_close - log_open.shift(1)\n",
    "data['log_close1_2'] = log_close - log_close.shift(1)\n",
    "data['log_close1_3'] = log_close - log_high.shift(1)\n",
    "data['log_close1_4'] = log_close - log_low.shift(1)\n",
    "\n",
    "data['log_close2_1'] = log_close - log_open.shift(2)\n",
    "data['log_close2_2'] = log_close - log_close.shift(2)\n",
    "data['log_close2_3'] = log_close - log_high.shift(2)\n",
    "data['log_close2_4'] = log_close - log_low.shift(2)\n",
    "\n",
    "data['log_close3_1'] = log_close - log_open.shift(3)\n",
    "data['log_close3_2'] = log_close - log_close.shift(3)\n",
    "data['log_close3_3'] = log_close - log_high.shift(3)\n",
    "data['log_close3_4'] = log_close - log_low.shift(3)\n",
    "\n",
    "data['log_close4_1'] = log_close - log_open.shift(4)\n",
    "data['log_close4_2'] = log_close - log_close.shift(4)\n",
    "data['log_close4_3'] = log_close - log_high.shift(4)\n",
    "data['log_close4_4'] = log_close - log_low.shift(4)\n",
    "\n",
    "data['log_close5_1'] = log_close - log_open.shift(5)\n",
    "data['log_close5_2'] = log_close - log_close.shift(5)\n",
    "data['log_close5_3'] = log_close - log_high.shift(5)\n",
    "data['log_close5_4'] = log_close - log_low.shift(5)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "data['log_high1_1'] = log_high - log_open.shift(1)\n",
    "data['log_high1_2'] = log_high - log_close.shift(1)\n",
    "data['log_high1_3'] = log_high - log_high.shift(1)\n",
    "data['log_high1_4'] = log_high - log_low.shift(1)\n",
    "\n",
    "data['log_high2_1'] = log_high - log_open.shift(2)\n",
    "data['log_high2_2'] = log_high - log_close.shift(2)\n",
    "data['log_high2_3'] = log_high - log_high.shift(2)\n",
    "data['log_high2_4'] = log_high - log_low.shift(2)\n",
    "\n",
    "data['log_high3_1'] = log_high - log_open.shift(3)\n",
    "data['log_high3_2'] = log_high - log_close.shift(3)\n",
    "data['log_high3_3'] = log_high - log_high.shift(3)\n",
    "data['log_high3_4'] = log_high - log_low.shift(3)\n",
    "\n",
    "data['log_high4_1'] = log_high - log_open.shift(4)\n",
    "data['log_high4_2'] = log_high - log_close.shift(4)\n",
    "data['log_high4_3'] = log_high - log_high.shift(4)\n",
    "data['log_high4_4'] = log_high - log_low.shift(4)\n",
    "\n",
    "data['log_high5_1'] = log_high - log_open.shift(5)\n",
    "data['log_high5_2'] = log_high - log_close.shift(5)\n",
    "data['log_high5_3'] = log_high - log_high.shift(5)\n",
    "data['log_high5_4'] = log_high - log_low.shift(5)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "data['log_low1_1'] = log_low - log_open.shift(1)\n",
    "data['log_low1_2'] = log_low - log_close.shift(1)\n",
    "data['log_low1_3'] = log_low - log_high.shift(1)\n",
    "data['log_low1_4'] = log_low - log_low.shift(1)\n",
    "\n",
    "data['log_low2_1'] = log_low - log_open.shift(2)\n",
    "data['log_low2_2'] = log_low - log_close.shift(2)\n",
    "data['log_low2_3'] = log_low - log_high.shift(2)\n",
    "data['log_low2_4'] = log_low - log_low.shift(2)\n",
    "\n",
    "data['log_low3_1'] = log_low - log_open.shift(3)\n",
    "data['log_low3_2'] = log_low - log_close.shift(3)\n",
    "data['log_low3_3'] = log_low - log_high.shift(3)\n",
    "data['log_low3_4'] = log_low - log_low.shift(3)\n",
    "\n",
    "data['log_low4_1'] = log_low - log_open.shift(4)\n",
    "data['log_low4_2'] = log_low - log_close.shift(4)\n",
    "data['log_low4_3'] = log_low - log_high.shift(4)\n",
    "data['log_low4_4'] = log_low - log_low.shift(4)\n",
    "\n",
    "data['log_low5_1'] = log_low - log_open.shift(5)\n",
    "data['log_low5_2'] = log_low - log_close.shift(5)\n",
    "data['log_low5_3'] = log_low - log_high.shift(5)\n",
    "data['log_low5_4'] = log_low - log_low.shift(5)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "data = data[5:]\n",
    "data.to_csv(file_80, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8048fe8e55c40a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:45:56.039445Z",
     "start_time": "2025-04-05T04:45:53.651068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file OK\n",
      "Test file OK\n"
     ]
    }
   ],
   "source": [
    "file_name = file_80\n",
    "df1 = pd.read_csv(file_name)\n",
    "\n",
    "data = df1.iloc[:, 1:].values\n",
    "\n",
    "### 预分配结果数组\n",
    "n_samples, n_features = data.shape\n",
    "r_features = n_features * roll  ### 80*5=400\n",
    "result = np.empty((n_samples, r_features + 1)) * np.nan\n",
    "result[:, 0] = 2\n",
    "\n",
    "###  批量填充数据\n",
    "for i in range(roll):\n",
    "    start = i * n_features + 1\n",
    "    end = start + n_features\n",
    "    result[i:, start:end] = data[:-i or None]  \n",
    "\n",
    "####  一次性创建新DataFrame\n",
    "columns = [f\"A{i}\" for i in range(r_features + 1)]\n",
    "df2 = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "### Train\n",
    "df_train = df2[30:Train + 30]\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train.to_csv(file_400_Train, index=False)\n",
    "print('Train file OK')\n",
    "\n",
    "### Test\n",
    "df_test = df2[-Test-1:-1]\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_test = df_test.drop(columns='A0')  ### 删除A0这一列\n",
    "df_test.to_csv(file_400_Test, index=False)\n",
    "print('Test file OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffc8f02650f84b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:46:27.173384Z",
     "start_time": "2025-04-05T04:46:22.389934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train + Test PCA file OK\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################\n",
    "#复制训练集，加到测试的前面和后面，为了应对PCA降维\n",
    "\n",
    "data_Train = pd.read_csv(file_400_Train)\n",
    "data_Train = data_Train.drop('A0', axis=1)  ### 删除A0这列\n",
    "data = pd.concat([data_Train, df_test, data_Train], ignore_index=True)\n",
    "\n",
    "data.to_csv(file_400_Train_3, index=False)\n",
    "print('Train + Test PCA file OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224ff42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18ad529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label OK\n"
     ]
    }
   ],
   "source": [
    "####### 硬标签 /方向概率标签 / 方向强度标签  #########################################\n",
    "\n",
    "file_name = file_400_Train\n",
    "df_400 = pd.read_csv(file_name)\n",
    "\n",
    "if df_400.iloc[0, 22] == df_label.iloc[label, 1]:\n",
    "    \n",
    "    ### 硬标签：state / 方向概率标签：state_p1 / 方向强度标签：state_p2\n",
    "    \n",
    "    label_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state']\n",
    "    label_copy = label_copy.reset_index(drop=True)\n",
    "    df_400['A0'] = label_copy\n",
    "    df_400.to_csv(file_400_Train, index=False)\n",
    "    print('Label OK')\n",
    "else:\n",
    "    print('Label NG !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b89db935a652e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T04:46:34.025025Z",
     "start_time": "2025-04-05T04:46:31.553474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label OK\n"
     ]
    }
   ],
   "source": [
    "#####  双概率标签 A0 B0  ###############################################################\n",
    "\n",
    "file_name = file_400_Train\n",
    "df_400 = pd.read_csv(file_name)\n",
    "\n",
    "if df_400.iloc[0, 22] == df_label.iloc[label, 1]:\n",
    "    \n",
    "    labelA_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_1_prob']    \n",
    "    labelB_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_0_prob']   \n",
    "    labelA_copy = labelA_copy.reset_index(drop=True)\n",
    "    labelB_copy = labelB_copy.reset_index(drop=True)\n",
    "    df_400['A0'] = labelA_copy\n",
    "    df_400['B0'] = labelB_copy\n",
    "    df_400.to_csv(file_400_Train, index=False)\n",
    "    print('Label OK')\n",
    "else:\n",
    "    print('Label NG !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178996e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label OK\n"
     ]
    }
   ],
   "source": [
    "####### 标签组合：硬标签 + 方向强度标签  #########################################\n",
    "\n",
    "file_name = file_400_Train\n",
    "df_400 = pd.read_csv(file_name)\n",
    "\n",
    "if df_400.iloc[0, 22] == df_label.iloc[label, 1]:\n",
    "    \n",
    "    labelA_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state']    \n",
    "    labelB_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_p2']   \n",
    "    labelA_copy = labelA_copy.reset_index(drop=True)\n",
    "    labelB_copy = labelB_copy.reset_index(drop=True)\n",
    "    df_400['A0'] = labelA_copy\n",
    "    df_400['B0'] = labelB_copy\n",
    "    df_400.to_csv(file_400_Train, index=False)\n",
    "    print('Label OK')\n",
    "else:\n",
    "    print('Label NG !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc4f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label OK\n"
     ]
    }
   ],
   "source": [
    "####### 标签组合：双概率标签 + 方向强度标签  #########################################\n",
    "\n",
    "file_name = file_400_Train\n",
    "df_400 = pd.read_csv(file_name)\n",
    "\n",
    "if df_400.iloc[0, 22] == df_label.iloc[label, 1]:\n",
    "    \n",
    "    labelA_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_1_prob']    \n",
    "    labelB_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_0_prob']   \n",
    "    labelC_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_p2']   \n",
    "    labelA_copy = labelA_copy.reset_index(drop=True)\n",
    "    labelB_copy = labelB_copy.reset_index(drop=True)\n",
    "    labelC_copy = labelC_copy.reset_index(drop=True)\n",
    "    df_400['A0'] = labelA_copy\n",
    "    df_400['B0'] = labelB_copy\n",
    "    df_400['C0'] = labelC_copy\n",
    "    df_400.to_csv(file_400_Train, index=False)\n",
    "    print('Label OK')\n",
    "else:\n",
    "    print('Label NG !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3c372b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label OK\n"
     ]
    }
   ],
   "source": [
    "####### 标签组合：硬标签 + 方向概率标签  #########################################\n",
    "\n",
    "file_name = file_400_Train\n",
    "df_400 = pd.read_csv(file_name)\n",
    "\n",
    "if df_400.iloc[0, 22] == df_label.iloc[label, 1]:\n",
    "    \n",
    "    labelA_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state']    \n",
    "    labelB_copy = df_label.loc[(label+move):(len(df_400)+label+move-1),'state_p1']   \n",
    "    labelA_copy = labelA_copy.reset_index(drop=True)\n",
    "    labelB_copy = labelB_copy.reset_index(drop=True)\n",
    "    df_400['A0'] = labelA_copy\n",
    "    df_400['B0'] = labelB_copy\n",
    "    df_400.to_csv(file_400_Train, index=False)\n",
    "    print('Label OK')\n",
    "else:\n",
    "    print('Label NG !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c658fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95872e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66496d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New_World_2",
   "language": "python",
   "name": "new_world_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
