import copy
import gc
import os
from typing import List, Optional, Union

import numba as nb
import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier, LGBMRegressor
from sklearn.model_selection import GridSearchCV
from tqdm.auto import tqdm

from src.utils.drop_na import drop_na_and_align_x_and_y

nb.set_num_threads(max(1, os.cpu_count() - 1))


# numbaåŠ é€Ÿçš„ç›¸å…³ç³»æ•°è®¡ç®—
@nb.njit(parallel=True)
def fast_corrwith_numba(X_values: np.ndarray, y_values: np.ndarray) -> np.ndarray:
    """
    ä½¿ç”¨numbaåŠ é€Ÿè®¡ç®—Xçš„æ¯ä¸€åˆ—ä¸yçš„ç›¸å…³ç³»æ•°

    Parameters
    ----------
    X_values: np.ndarray
        ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)
    y_values: np.ndarray
        ç›®æ ‡å‘é‡ï¼Œå½¢çŠ¶ä¸º(n_samples,)

    Returns
    -------
    np.ndarray
        æ¯ä¸ªç‰¹å¾ä¸yçš„ç›¸å…³ç³»æ•°çš„ç»å¯¹å€¼ï¼Œå½¢çŠ¶ä¸º(n_features,)
    """
    n_features = X_values.shape[1]
    result = np.zeros(n_features)

    # è®¡ç®—yçš„æ ‡å‡†å·®
    y_mean = np.mean(y_values)
    y_std = np.std(y_values)

    if y_std == 0:
        return result  # å¦‚æœyæ˜¯å¸¸æ•°ï¼Œè¿”å›å…¨é›¶æ•°ç»„

    # æ ‡å‡†åŒ–y (ä¸€æ¬¡æ€§è®¡ç®—)
    y_norm = (y_values - y_mean) / y_std

    # å¯¹æ¯åˆ—è®¡ç®—ç›¸å…³ç³»æ•° (å¹¶è¡Œ)
    for i in nb.prange(n_features):
        x = X_values[:, i]
        x_mean = np.mean(x)
        x_std = np.std(x)

        if x_std == 0:
            result[i] = 0  # å¦‚æœxæ˜¯å¸¸æ•°ï¼Œç›¸å…³ç³»æ•°ä¸º0
            continue

        # æ ‡å‡†åŒ–xå¹¶è®¡ç®—ç›¸å…³ç³»æ•°
        x_norm = (x - x_mean) / x_std
        corr = np.mean(x_norm * y_norm)
        result[i] = abs(corr)  # å–ç»å¯¹å€¼

    return result


# åŸºäºéšæœºæ£®æ—çš„RFCQç‰¹å¾é€‰æ‹©å™¨å®ç°
class RFCQSelector:
    """
    åŸºäºéšæœºæ£®æ—çš„RFCQç‰¹å¾é€‰æ‹©å™¨ï¼Œå…³é”®éƒ¨åˆ†ä½¿ç”¨numbaåŠ é€Ÿ

    RFCQ = Random Forest for relevance, Correlation for redundancy, Quotient for combining

    Parameters
    ----------
    max_features: int, é»˜è®¤=None
        è¦é€‰æ‹©çš„ç‰¹å¾æ•°é‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™é»˜è®¤ä¸ºç‰¹å¾æ€»æ•°çš„20%ã€‚

    task_type: str, é»˜è®¤='auto'
        ä»»åŠ¡ç±»å‹ï¼š'classification', 'regression' æˆ– 'auto'ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

    scoring: str, é»˜è®¤=None
        è¯„ä¼°éšæœºæ£®æ—æ€§èƒ½çš„æŒ‡æ ‡ã€‚å¦‚æœä¸ºNoneï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨è®¾ç½®ï¼š
        - åˆ†ç±»ä»»åŠ¡ï¼š'roc_auc'
        - å›å½’ä»»åŠ¡ï¼š'neg_root_mean_squared_error'

    cv: int, é»˜è®¤=3
        äº¤å‰éªŒè¯çš„æŠ˜æ•°ã€‚

    param_grid: dict, é»˜è®¤=None
        éšæœºæ£®æ—çš„è¶…å‚æ•°ç½‘æ ¼ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ç½‘æ ¼ {"max_depth": [1, 2, 3, 4]}ã€‚

    verbose: bool, é»˜è®¤=True
        æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡å’Œè¯¦ç»†ä¿¡æ¯ã€‚

    random_state: int, é»˜è®¤=None
        éšæœºç§å­ï¼Œç”¨äºéšæœºæ£®æ—çš„åˆå§‹åŒ–ã€‚

    n_jobs: int, é»˜è®¤=None
        å¹¶è¡Œä»»åŠ¡æ•°ã€‚Noneè¡¨ç¤º1ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¤„ç†å™¨ã€‚

    Attributes
    ----------
    features_to_drop_: list
        è®­ç»ƒåè¦åˆ é™¤çš„ç‰¹å¾åˆ—è¡¨

    variables_: list
        è€ƒè™‘çš„ç‰¹å¾åˆ—è¡¨

    relevance_: numpy.ndarray
        æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
    """

    def __init__(
        self,
        max_features: Optional[int] = None,
        task_type: str = "auto",
        scoring: Optional[str] = None,
        cv: int = 3,
        param_grid: Optional[dict] = None,
        verbose: bool = True,
        random_state: Optional[int] = 42,
        n_jobs: Optional[int] = max(os.cpu_count() - 1, 1),
    ):
        self.max_features = max_features
        self.task_type = task_type

        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨è®¾ç½®è¯„åˆ†æŒ‡æ ‡
        if scoring is None:
            if task_type == "classification":
                self.scoring = "roc_auc"
            elif task_type == "regression":
                self.scoring = "neg_root_mean_squared_error"
            else:  # auto
                self.scoring = None  # å°†åœ¨fitæ—¶æ ¹æ®æ•°æ®ç±»å‹å†³å®š
        else:
            self.scoring = scoring

        self.cv = cv
        self.param_grid = param_grid
        self.verbose = verbose
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.features_to_drop_ = None
        self.variables_ = None
        self.relevance_ = None

    def _find_numerical_variables(self, X: pd.DataFrame) -> List[str]:
        """
        æ‰¾å‡ºæ•°æ®æ¡†ä¸­çš„æ•°å€¼å‹å˜é‡
        """
        return X.select_dtypes(include=["number"]).columns.tolist()

    def _calculate_relevance(self, X: pd.DataFrame, y: pd.Series) -> np.ndarray:
        """
        è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§ï¼ˆä½¿ç”¨éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ï¼‰
        """
        X_values = X.values
        y_values = y.values

        # è‡ªåŠ¨æ£€æµ‹ä»»åŠ¡ç±»å‹
        if self.task_type == "auto":
            unique_values = len(np.unique(y_values))
            # å¦‚æœå”¯ä¸€å€¼æ•°é‡å°äº5æˆ–è€…æ¯”æ ·æœ¬æ•°çš„1%è¿˜å°‘ï¼Œè§†ä¸ºåˆ†ç±»ä»»åŠ¡
            is_classification = unique_values < min(5, len(y_values) * 0.01)
        else:
            is_classification = self.task_type == "classification"

        # ç¡®å®šscoringï¼ˆå¦‚æœåœ¨æ„é€ æ—¶æœªè®¾ç½®ï¼‰
        if self.scoring is None:
            scoring = "roc_auc" if is_classification else "neg_root_mean_squared_error"
        else:
            scoring = self.scoring

        # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨LightGBMéšæœºæ£®æ—æ¨¡å¼ï¼‰
        if is_classification:
            model = LGBMClassifier(
                boosting_type="rf",  # å¯ç”¨éšæœºæ£®æ—æ¨¡å¼
                n_estimators=100,
                num_leaves=31,  # é»˜è®¤å€¼ï¼Œå¯è¢«GridSearchCVè¦†ç›–
                subsample=0.632,  # RF bootstrapé‡‡æ ·ç‡
                subsample_freq=1,  # æ¯æ£µæ ‘éƒ½é‡‡æ ·
                colsample_bytree=1.0,  # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
                class_weight="balanced",
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                verbose=-1,  # ç¦ç”¨LightGBMå†…éƒ¨æ—¥å¿—
                # M4 Proæ€§èƒ½ä¼˜åŒ–
                max_bin=63,  # å‡å°‘binæ•°é‡ï¼Œåœ¨Apple Siliconä¸Šæ˜¾è‘—æé€Ÿ
                histogram_pool_size=512,  # é™åˆ¶histogramç¼“å­˜å¤§å°(MB)
            )
        else:
            model = LGBMRegressor(
                boosting_type="rf",  # å¯ç”¨éšæœºæ£®æ—æ¨¡å¼
                n_estimators=100,
                num_leaves=31,  # é»˜è®¤å€¼ï¼Œå¯è¢«GridSearchCVè¦†ç›–
                subsample=0.632,  # RF bootstrapé‡‡æ ·ç‡
                subsample_freq=1,  # æ¯æ£µæ ‘éƒ½é‡‡æ ·
                colsample_bytree=1.0,  # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                verbose=-1,  # ç¦ç”¨LightGBMå†…éƒ¨æ—¥å¿—
                # M4 Proæ€§èƒ½ä¼˜åŒ–
                max_bin=63,  # å‡å°‘binæ•°é‡ï¼Œåœ¨Apple Siliconä¸Šæ˜¾è‘—æé€Ÿ
                histogram_pool_size=512,  # é™åˆ¶histogramç¼“å­˜å¤§å°(MB)
            )

        # è®¾ç½®å‚æ•°ç½‘æ ¼
        if self.param_grid:
            param_grid = self.param_grid
        else:
            # ä½¿ç”¨num_leavesæ›¿ä»£max_depthï¼Œå¯¹åº”å…³ç³»: 2^(depth+1)-1
            # å‡å°‘ç½‘æ ¼æœç´¢ç©ºé—´ä»¥æé€Ÿ
            param_grid = {"num_leaves": [31, 63]}  # å¯¹åº”max_depth 3,4

        # ç½‘æ ¼æœç´¢
        cv_model = GridSearchCV(
            model, cv=self.cv, scoring=scoring, param_grid=param_grid
        )

        cv_model.fit(X_values, y_values)

        # è·å–ç‰¹å¾é‡è¦æ€§
        relevance = cv_model.best_estimator_.feature_importances_.copy()

        # ğŸ”§ æ˜¾å¼æ¸…ç† GridSearchCV çš„å†…éƒ¨èµ„æº
        # å…ˆåˆ é™¤æœ€ä½³ä¼°è®¡å™¨ï¼ˆåŒ…å«è®­ç»ƒæ•°æ®å¼•ç”¨ï¼‰
        del cv_model.best_estimator_
        # åˆ é™¤ CV ç»“æœï¼ˆå¯èƒ½åŒ…å«å¤§é‡ä¸­é—´æ•°æ®ï¼‰
        if hasattr(cv_model, 'cv_results_'):
            del cv_model.cv_results_
        # åˆ é™¤æ•´ä¸ª GridSearchCV å¯¹è±¡
        del cv_model
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        return relevance

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "RFCQSelector":
        """
        è®­ç»ƒRFCQç‰¹å¾é€‰æ‹©å™¨

        Parameters
        ----------
        X: pandas.DataFrame
            ç‰¹å¾æ•°æ®æ¡†

        y: pandas.Series
            ç›®æ ‡å˜é‡

        Returns
        -------
        self
        """
        # ç¡®ä¿è¾“å…¥æ˜¯pandaså¯¹è±¡
        if not isinstance(X, pd.DataFrame):
            raise TypeError("Xå¿…é¡»æ˜¯pandas.DataFrame")
        if not isinstance(y, pd.Series):
            y = pd.Series(y)

        # å¯¹é½xä¸yçš„é•¿åº¦å¹¶å»é™¤xå¼€å¤´å¯èƒ½å­˜åœ¨çš„ç©ºå€¼
        X, y = drop_na_and_align_x_and_y(X, y)

        # æ‰¾å‡ºæ•°å€¼å‹å˜é‡
        if self.verbose:
            print("â¤ è¯†åˆ«æ•°å€¼å‹å˜é‡...")
        self.variables_ = self._find_numerical_variables(X)

        if len(self.variables_) < 2:
            raise ValueError("è‡³å°‘éœ€è¦2ä¸ªæ•°å€¼å‹ç‰¹å¾æ¥æ‰§è¡Œç‰¹å¾é€‰æ‹©")

        # è®¡ç®—ç›¸å…³æ€§
        if self.verbose:
            print("â¤ è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§(ä½¿ç”¨éšæœºæ£®æ—)...")
        X_numeric = X[self.variables_]
        self.relevance_ = self._calculate_relevance(X_numeric, y)

        # é¢„å…ˆè·å–æ‰€æœ‰ç‰¹å¾æ•°æ®
        X_data = X[self.variables_].values

        # åˆå§‹åŒ–
        relevance = self.relevance_.copy()
        remaining = copy.deepcopy(self.variables_)

        # æ‰¾å‡ºæœ€ç›¸å…³çš„ç‰¹å¾
        n = np.argmax(relevance)
        top_feature = remaining[n]

        if self.verbose:
            print(
                f"âœ“ é€‰æ‹©ç¬¬1ä¸ªç‰¹å¾: {top_feature} (æœ€å¤§é‡è¦æ€§: {self.relevance_[n]:.4f})"
            )

        # æ›´æ–°ç‰¹å¾åˆ—è¡¨
        selected = [top_feature]
        remaining.remove(top_feature)
        relevance = np.delete(relevance, n)

        # ç‰¹å¾çš„ç´¢å¼•æ˜ å°„
        feature_to_idx = {f: i for i, f in enumerate(self.variables_)}

        # è®¡ç®—å…¶ä»–ç‰¹å¾ä¸æœ€ä½³ç‰¹å¾çš„å†—ä½™åº¦
        if self.verbose:
            print("â¤ è®¡ç®—ç‰¹å¾å†—ä½™åº¦...")
        top_feature_idx = feature_to_idx[top_feature]
        remaining_indices = [feature_to_idx[f] for f in remaining]
        X_remaining = X_data[:, remaining_indices]
        y_values = X_data[:, top_feature_idx]
        redundance = fast_corrwith_numba(X_remaining, y_values)

        # ç¡®å®šè¦é€‰æ‹©çš„ç‰¹å¾æ•°é‡
        if self.max_features is None:
            n_to_select = max(1, int(0.2 * len(self.variables_)))
        else:
            n_to_select = min(self.max_features, len(self.variables_))

        # ç¬¬ä¸€è½®å·²ç»é€‰äº†ä¸€ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥å‡1
        n_to_select = n_to_select - 1

        if self.verbose:
            print(
                f"â¤ æ€»è®¡é€‰æ‹©{n_to_select + 1}ä¸ªç‰¹å¾ (å·²é€‰æ‹©1ä¸ªï¼Œè¿˜éœ€é€‰æ‹©{n_to_select}ä¸ª)..."
            )
            print("â¤ å¼€å§‹MRMRè¿­ä»£é€‰æ‹©è¿‡ç¨‹...")

        # ä¸»å¾ªç¯ï¼šè¿­ä»£é€‰æ‹©ç‰¹å¾
        for i in tqdm(
            range(n_to_select),
            disable=not self.verbose,
            desc="é€‰æ‹©ç‰¹å¾",
            unit="ç‰¹å¾",
            ncols=100,
        ):
            if len(remaining) == 0:
                break

            if i == 0:
                # ç¬¬ä¸€è½®è¿­ä»£ï¼Œå†—ä½™åº¦æ˜¯ä¸€ç»´çš„
                # è®¡ç®—MRMR
                eps = 1e-10
                safe_redundance = np.maximum(redundance, eps)
                mrmr_scores = relevance / safe_redundance
                n = np.argmax(mrmr_scores)

                # æ›´æ–°ç‰¹å¾åˆ—è¡¨
                feature = remaining[n]
                feature_idx = feature_to_idx[feature]
                selected.append(feature)
                remaining.remove(feature)

                # æ›´æ–°ç´¢å¼•
                remaining_indices.remove(feature_idx)

                relevance = np.delete(relevance, n)
                redundance = np.delete(redundance, n)
            else:
                # åç»­è¿­ä»£ï¼Œå†—ä½™åº¦æ˜¯äºŒç»´çš„
                # è®¡ç®—å¹³å‡å†—ä½™åº¦
                mean_redundance = np.mean(redundance, axis=0)

                # è®¡ç®—MRMR
                eps = 1e-10
                safe_redundance = np.maximum(mean_redundance, eps)
                mrmr_scores = relevance / safe_redundance
                n = np.argmax(mrmr_scores)

                # æ›´æ–°ç‰¹å¾åˆ—è¡¨
                feature = remaining[n]
                feature_idx = feature_to_idx[feature]
                selected.append(feature)
                remaining.remove(feature)

                # æ›´æ–°ç´¢å¼•
                remaining_indices.remove(feature_idx)

                relevance = np.delete(relevance, n)
                redundance = np.delete(redundance, n, axis=1)

            # å¦‚æœå·²ç»é€‰å®Œäº†æ‰€æœ‰ç‰¹å¾ï¼Œé€€å‡ºå¾ªç¯
            if len(remaining) == 0:
                break

            # è®¡ç®—æ–°çš„å†—ä½™åº¦
            X_remaining = X_data[:, remaining_indices]
            y_values = X_data[:, feature_idx]
            new_redundance = fast_corrwith_numba(X_remaining, y_values)

            # ç¬¬ä¸€æ¬¡æ·»åŠ æ—¶ï¼Œåˆ›å»º2Dæ•°ç»„
            if i == 0:
                redundance = np.vstack(
                    [redundance[np.newaxis, :], new_redundance[np.newaxis, :]]
                )
            else:
                # æ·»åŠ æ–°çš„å†—ä½™åº¦
                redundance = np.vstack([redundance, new_redundance[np.newaxis, :]])

        # è®°å½•è¦ä¸¢å¼ƒçš„ç‰¹å¾
        self.features_to_drop_ = [f for f in self.variables_ if f not in selected]

        if self.verbose:
            total_features = len(self.variables_)
            selected_count = len(selected)
            dropped_count = len(self.features_to_drop_)
            print(
                f"\nâœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼šä»{total_features}ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†{selected_count}ä¸ªï¼Œèˆå¼ƒäº†{dropped_count}ä¸ª"
            )

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        è½¬æ¢æ•°æ®ï¼Œåªä¿ç•™é€‰ä¸­çš„ç‰¹å¾

        Parameters
        ----------
        X: pandas.DataFrame
            è¾“å…¥æ•°æ®

        Returns
        -------
        pandas.DataFrame
            åªåŒ…å«é€‰ä¸­ç‰¹å¾çš„æ•°æ®æ¡†
        """
        if self.features_to_drop_ is None:
            raise ValueError("åœ¨ä½¿ç”¨transformä¹‹å‰å¿…é¡»å…ˆè°ƒç”¨fit")

        return X.drop(columns=self.features_to_drop_)

    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """
        è®­ç»ƒé€‰æ‹©å™¨å¹¶è½¬æ¢æ•°æ®

        Parameters
        ----------
        X: pandas.DataFrame
            ç‰¹å¾æ•°æ®æ¡†

        y: pandas.Series
            ç›®æ ‡å˜é‡

        Returns
        -------
        pandas.DataFrame
            åªåŒ…å«é€‰ä¸­ç‰¹å¾çš„æ•°æ®æ¡†
        """
        return self.fit(X, y).transform(X)

    def get_support(self, indices: bool = False) -> Union[np.ndarray, List[int]]:
        """
        è·å–æ‰€é€‰ç‰¹å¾çš„å¸ƒå°”æ©ç æˆ–ç´¢å¼•

        Parameters
        ----------
        indices: bool, é»˜è®¤=False
            å¦‚æœä¸ºTrueï¼Œè¿”å›ç‰¹å¾ç´¢å¼•ï¼Œå¦åˆ™è¿”å›å¸ƒå°”æ©ç 

        Returns
        -------
        numpy.ndarray æˆ– List[int]
            ç‰¹å¾é€‰æ‹©æ©ç æˆ–ç´¢å¼•
        """
        if self.features_to_drop_ is None:
            raise ValueError("åœ¨ä½¿ç”¨get_supportä¹‹å‰å¿…é¡»å…ˆè°ƒç”¨fit")

        mask = np.ones(len(self.variables_), dtype=bool)
        for f in self.features_to_drop_:
            idx = self.variables_.index(f)
            mask[idx] = False

        if indices:
            return np.where(mask)[0].tolist()
        else:
            return mask
