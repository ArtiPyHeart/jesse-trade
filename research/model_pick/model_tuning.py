import gc

import lightgbm as lgb
import numpy as np
import optuna
import pandas as pd
from jesse.helpers import date_to_timestamp
from optuna.integration import LightGBMPruningCallback
from sklearn.metrics import f1_score
from sklearn.model_selection import KFold, StratifiedKFold

from src.utils.drop_na import drop_na_and_align_x_and_y
from .feature_select import FeatureSelector

METRIC = "f1"


def eval_metric(preds, eval_dataset):
    metric_name = METRIC
    y_true = eval_dataset.get_label()
    value = f1_score(y_true, preds > 0.5, average="weighted")
    higher_better = True
    return metric_name, value, higher_better


class ModelTuning:
    def __init__(self, train_test_split_date: str, x: pd.DataFrame, y: np.ndarray):
        self.split_date = train_test_split_date

        self.X = x
        self.Y = y

        train_mask = self.X.index.to_numpy() < date_to_timestamp(self.split_date)

        self.train_X = x[train_mask]
        self.train_Y = y[train_mask]

        assert len(self.train_X) == len(self.train_Y)

    def tuning_classifier(
        self, selector: FeatureSelector, feature_names: list[str]
    ) -> tuple[dict, float]:
        all_feats = selector.get_all_features(self.train_X)[feature_names]
        print(f"{len(feature_names)} features selected")

        x, y = drop_na_and_align_x_and_y(all_feats, self.train_Y)

        # LightGBM prefers contiguous float32 arrays; cache once to reuse across trials
        x = np.ascontiguousarray(x.to_numpy(dtype=np.float32))

        # å›ºå®šmax_binå‚æ•°ï¼Œä½¿ç”¨ free_raw_data=True é‡Šæ”¾åŸå§‹æ•°æ®é¿å…å†…å­˜æ³„æ¼
        dtrain = lgb.Dataset(x, y, free_raw_data=True, params={"max_bin": 255})
        cv_folds = list(
            StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(x, y)
        )

        def objective(trial):
            boosting_type = trial.suggest_categorical("boosting", ["gbdt", "dart"])
            params = {
                "objective": "binary",
                # ä¸åœ¨paramsä¸­æŒ‡å®šmetricï¼Œå› ä¸ºä½¿ç”¨fevalæ—¶ä¼šå†²çª
                "num_threads": -1,
                "verbose": -1,
                "is_unbalance": trial.suggest_categorical(
                    "is_unbalance", [True, False]
                ),
                "extra_trees": trial.suggest_categorical("extra_trees", [True, False]),
                "boosting": boosting_type,
                "learning_rate": trial.suggest_float(
                    "learning_rate", 0.02, 0.1, log=True
                ),
                "num_leaves": trial.suggest_int("num_leaves", 31, 512),
                "max_depth": trial.suggest_int("max_depth", 10, 50),
                "min_gain_to_split": trial.suggest_float(
                    "min_gain_to_split", 1e-8, 1, log=True
                ),
                "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 200),
                "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
                "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
                "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
                "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
                "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
                # ç§»é™¤è¿‡äºæ¿€è¿›çš„æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼Œä¼˜å…ˆä¿è¯æ¨¡å‹è´¨é‡
                "feature_pre_filter": False,
            }
            # å¦‚æœä½¿ç”¨ dartï¼Œæ·»åŠ  dart ç‰¹æœ‰å‚æ•°
            if boosting_type == "dart":
                params["drop_rate"] = trial.suggest_float("drop_rate", 0.1, 0.5)
                params["skip_drop"] = trial.suggest_float("skip_drop", 0.1, 0.5)

            num_boost_round = trial.suggest_int("num_boost_round", 300, 1500)
            # æ³¨æ„ï¼šä½¿ç”¨fevalæ—¶ï¼ŒLightGBMPruningCallbackéœ€è¦metricåç§°è€Œé"metric-mean"æ ¼å¼
            # å®é™…callbackæ¥æ”¶çš„æ˜¯('valid', 'f1', value, is_higher_better, std)æ ¼å¼
            pruning_cb = LightGBMPruningCallback(trial, METRIC)
            callbacks = [
                lgb.early_stopping(stopping_rounds=150, verbose=False),
                pruning_cb,
            ]
            model_res = lgb.cv(
                params,
                dtrain,
                num_boost_round=num_boost_round,
                folds=cv_folds,
                feval=eval_metric,
                callbacks=callbacks,
            )
            return model_res[f"valid {METRIC}-mean"][-1]

        study = optuna.create_study(
            direction="maximize",
            pruner=optuna.pruners.HyperbandPruner(),
            sampler=optuna.samplers.TPESampler(
                n_startup_trials=100,
                multivariate=True,
                constant_liar=True,
                warn_independent_sampling=False,
            ),
        )
        # è®¾ç½® Optuna æ—¥å¿—çº§åˆ«ä¸ºè­¦å‘Šï¼Œéšè—è¯¦ç»†æ—¥å¿—
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        # ä½¿ç”¨ show_progress_bar æ˜¾ç¤ºè¿›åº¦æ¡
        # n_jobs=1 åœ¨M4 Proä¸Šé¿å…è¿‡åº¦å¹¶è¡Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™
        study.optimize(
            objective, n_trials=300, n_jobs=1, show_progress_bar=True
        )  # å¢åŠ è¯•éªŒæ¬¡æ•°

        params = {
            "objective": "binary",
            "num_threads": -1,
            "verbose": -1,
            **study.best_params,
        }
        best_value = study.best_value

        # ğŸ”§ æ˜¾å¼æ¸…ç† Optuna study å’Œ Datasetï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        del study
        del dtrain
        gc.collect()

        return params, best_value

    def tuning_regressor(
        self, selector: FeatureSelector, feature_names: list[str]
    ) -> tuple[dict, float]:
        all_feats = selector.get_all_features(self.train_X)[feature_names]
        print(f"{len(feature_names)} features selected")

        x, y = drop_na_and_align_x_and_y(all_feats, self.train_Y)

        # LightGBM prefers contiguous float32 arrays; cache once to reuse across trials
        x = np.ascontiguousarray(x.to_numpy(dtype=np.float32))
        # å›ºå®šmax_binå‚æ•°ï¼Œä½¿ç”¨ free_raw_data=True é‡Šæ”¾åŸå§‹æ•°æ®é¿å…å†…å­˜æ³„æ¼
        dtrain = lgb.Dataset(x, y, free_raw_data=True, params={"max_bin": 255})
        cv_folds = list(KFold(n_splits=5, shuffle=True, random_state=42).split(x))

        # é¢„è®¡ç®—è®­ç»ƒé›†æ ‡ç­¾çš„æ–¹å·®ï¼Œç”¨äºè®¡ç®—RÂ²
        y_var = np.var(y)

        # è‡ªå®šä¹‰RÂ²è¯„ä¼°å‡½æ•°ï¼ˆç”¨äºLightGBMçš„fevalå‚æ•°ï¼‰
        def r2_eval(preds, eval_dataset):
            """
            è®¡ç®—RÂ²åˆ†æ•°
            RÂ² = 1 - (MSE / Var(y))
            """
            y_true = eval_dataset.get_label()
            mse = np.mean((y_true - preds) ** 2)
            r2 = 1 - (mse / y_var)
            # è¿”å› (metric_name, metric_value, is_higher_better)
            return "r2", r2, True

        def objective(trial):
            boosting_type = trial.suggest_categorical("boosting", ["gbdt", "dart"])
            params = {
                "objective": "regression",
                # ä¸åœ¨paramsä¸­æŒ‡å®šmetricï¼Œä½¿ç”¨feval
                "num_threads": -1,
                "verbose": -1,
                "extra_trees": trial.suggest_categorical("extra_trees", [True, False]),
                "boosting": boosting_type,
                "num_leaves": trial.suggest_int("num_leaves", 31, 512),
                "max_depth": trial.suggest_int("max_depth", 10, 50),
                "min_gain_to_split": trial.suggest_float(
                    "min_gain_to_split", 1e-8, 1, log=True
                ),
                "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 200),
                "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
                "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
                # æ·»åŠ  regression ç‰¹æœ‰çš„å‚æ•°
                "min_sum_hessian_in_leaf": trial.suggest_float(
                    "min_sum_hessian_in_leaf", 1e-3, 10, log=True
                ),
                "learning_rate": trial.suggest_float(
                    "learning_rate", 0.02, 0.1, log=True
                ),
                "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
                "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
                "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
                # ç§»é™¤è¿‡äºæ¿€è¿›çš„æ€§èƒ½ä¼˜åŒ–å‚æ•°
                "feature_pre_filter": False,
            }

            # å¦‚æœä½¿ç”¨ dartï¼Œæ·»åŠ  dart ç‰¹æœ‰å‚æ•°
            if boosting_type == "dart":
                params["drop_rate"] = trial.suggest_float("drop_rate", 0.1, 0.5)
                params["skip_drop"] = trial.suggest_float("skip_drop", 0.1, 0.5)

            num_boost_round = trial.suggest_int("num_boost_round", 300, 1500)

            # ä½¿ç”¨LightGBMPruningCallbackç›‘æ§RÂ²æŒ‡æ ‡
            pruning_cb = LightGBMPruningCallback(trial, "r2")
            callbacks = [
                lgb.early_stopping(stopping_rounds=150, verbose=False),
                pruning_cb,
            ]
            model_res = lgb.cv(
                params,
                dtrain,
                num_boost_round=num_boost_round,
                folds=cv_folds,
                feval=r2_eval,  # ä½¿ç”¨è‡ªå®šä¹‰RÂ²è¯„ä¼°å‡½æ•°
                callbacks=callbacks,
            )
            # è¿”å›äº¤å‰éªŒè¯çš„å¹³å‡RÂ²åˆ†æ•°
            return model_res["valid r2-mean"][-1]

        study = optuna.create_study(
            direction="maximize",  # RÂ²éœ€è¦æœ€å¤§åŒ–
            pruner=optuna.pruners.HyperbandPruner(),
            sampler=optuna.samplers.TPESampler(
                n_startup_trials=100,
                multivariate=True,
                constant_liar=True,
                warn_independent_sampling=False,
            ),
        )
        # è®¾ç½® Optuna æ—¥å¿—çº§åˆ«ä¸ºè­¦å‘Šï¼Œéšè—è¯¦ç»†æ—¥å¿—
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        # ä½¿ç”¨ show_progress_bar æ˜¾ç¤ºè¿›åº¦æ¡
        # n_jobs=1 åœ¨M4 Proä¸Šé¿å…è¿‡åº¦å¹¶è¡Œå¯¼è‡´çš„æ€§èƒ½ä¸‹é™
        study.optimize(
            objective, n_trials=300, n_jobs=1, show_progress_bar=True
        )  # å¢åŠ è¯•éªŒæ¬¡æ•°

        params = {
            "objective": "regression",
            "num_threads": -1,
            "verbose": -1,
            **study.best_params,
        }
        best_value = study.best_value

        # ğŸ”§ æ˜¾å¼æ¸…ç† Optuna study å’Œ Datasetï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        del study
        del dtrain
        gc.collect()

        return params, best_value  # è¿”å›RÂ²å€¼
